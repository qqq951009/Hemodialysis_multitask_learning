{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"multitask_dnn_pytorch.ipynb","provenance":[{"file_id":"1RSZUQGALROCQJHZ_rys_infEKibrol0q","timestamp":1657942113070}],"collapsed_sections":[],"authorship_tag":"ABX9TyPLAWL6TrhhvYN/RVGprECk"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"gpuClass":"standard","accelerator":"GPU"},"cells":[{"cell_type":"code","source":["import tensorflow as tf\n","import pandas as pd\n","import numpy as np\n","from tensorflow import keras\n","from sklearn.preprocessing import StandardScaler\n","from sklearn import preprocessing\n","import torch\n","import torch.nn as nn\n","import torch.optim as optim\n","import numpy as np\n","from torch.utils.data import Dataset, DataLoader \n","import random\n","# Seed\n","seed = 0\n","torch.manual_seed(seed)\n","torch.cuda.manual_seed(seed)\n","torch.cuda.manual_seed_all(seed)\n","np.random.seed(seed)\n","random.seed(seed)\n","torch.backends.cudnn.benchmark = False\n","torch.backends.cudnn.deterministic = True"],"metadata":{"id":"KU9BqhKvb8oE","executionInfo":{"status":"ok","timestamp":1660021340665,"user_tz":-480,"elapsed":9513,"user":{"displayName":"洪睿甫","userId":"05593760976211923971"}}},"execution_count":1,"outputs":[]},{"cell_type":"code","source":["data = pd.read_excel('/content/HD_2019_M_2.xlsx')\n","all_nan = data.columns[data.isnull().sum() == 465].tolist()\n","data = data.drop(columns = all_nan)\n","trainset= {}"],"metadata":{"id":"dWY-5Eg-eWeZ","executionInfo":{"status":"ok","timestamp":1660023571164,"user_tz":-480,"elapsed":1221,"user":{"displayName":"洪睿甫","userId":"05593760976211923971"}}},"execution_count":28,"outputs":[]},{"cell_type":"code","source":["data"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":676},"id":"gJhUzfuLHYhg","executionInfo":{"status":"ok","timestamp":1660023579862,"user_tz":-480,"elapsed":1558,"user":{"displayName":"洪睿甫","userId":"05593760976211923971"}},"outputId":"03329176-3e14-42cc-c4cb-95349eafc788"},"execution_count":29,"outputs":[{"output_type":"execute_result","data":{"text/plain":["           ID        生日  性別      開始治療日期 HBsAg Anti-HCV                    合併症  \\\n","0     5004995  31/11/18   2   1996/8/28     N        N                  19-30   \n","1     3983367   35/2/15   1  1995/12/11     N        N                13-19-0   \n","2      400509  27/12/18   2   1993/3/16     N        N                12-0-25   \n","3     3468683   41/2/17   2   1995/7/30     N        N        8-11-12-19-25-0   \n","4     2419091  42/12/20   2  1994/12/12     N        N  10-22-30-33-13-19-5-7   \n","..        ...       ...  ..         ...   ...      ...                    ...   \n","460  11895105  30/11/28   1   2018/12/6     N        N            19-22-29-30   \n","461   1690080   28/1/24   2  2018/12/29     N        N                    NaN   \n","462  13944660   51/1/29   1    2019/1/5     N        N             1-25-29-30   \n","463   3846692   22/2/28   1   2019/1/22     N        N         10-19-22-29-30   \n","464  21217704   40/4/26   1    2016/7/1     N        N                  19-25   \n","\n","       PlanDate  透析方式  抗凝劑  ...  K (meq/l)  離子鈣 (mg/dl)  P (mg/dl)   URR  \\\n","0    20190130.0   3.0  0.0  ...        4.8         4.24        4.1  0.87   \n","1    20190130.0   3.0  1.0  ...        4.3         4.93        4.0  0.75   \n","2    20190130.0   3.0  1.0  ...        2.9         3.97        2.3  0.71   \n","3    20190130.0   3.0  2.0  ...        4.7         5.12        5.8  0.83   \n","4    20190130.0   3.0  0.0  ...        4.3         5.09        4.6  0.77   \n","..          ...   ...  ...  ...        ...          ...        ...   ...   \n","460  20190129.0   3.0  0.0  ...        3.6         4.05        5.7  0.73   \n","461  20190129.0   3.0  0.0  ...        3.1         3.98        4.5  0.72   \n","462         NaN   NaN  NaN  ...        NaN          NaN        NaN   NaN   \n","463         NaN   NaN  NaN  ...        NaN          NaN        NaN   NaN   \n","464         NaN   NaN  NaN  ...        NaN          NaN        NaN   NaN   \n","\n","     Kt/V (Gotch)  Kt/V  nPCR  TACurea  Cardiac/thoracic ratio (%)  \\\n","0            2.02  2.45  1.65     47.0                        0.53   \n","1            1.39  1.68  1.46     52.5                        0.48   \n","2            1.25  1.50  0.48     13.5                        0.52   \n","3            1.76  2.15  1.75     54.5                        0.42   \n","4            1.45  1.74  1.44     50.0                        0.62   \n","..            ...   ...   ...      ...                         ...   \n","460          1.32  1.61  0.85     28.5                         NaN   \n","461          1.26  1.42  1.57     61.0                         NaN   \n","462           NaN   NaN   NaN      NaN                         NaN   \n","463           NaN   NaN   NaN      NaN                         NaN   \n","464           NaN   NaN   NaN      NaN                         NaN   \n","\n","     Ca X P (mg2/dL2)  \n","0               34.77  \n","1               39.44  \n","2               18.26  \n","3               59.39  \n","4               46.83  \n","..                ...  \n","460             46.17  \n","461             35.82  \n","462               NaN  \n","463               NaN  \n","464               NaN  \n","\n","[465 rows x 49 columns]"],"text/html":["\n","  <div id=\"df-3b5e6046-d83a-47d6-bd60-f0c5f552f968\">\n","    <div class=\"colab-df-container\">\n","      <div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>ID</th>\n","      <th>生日</th>\n","      <th>性別</th>\n","      <th>開始治療日期</th>\n","      <th>HBsAg</th>\n","      <th>Anti-HCV</th>\n","      <th>合併症</th>\n","      <th>PlanDate</th>\n","      <th>透析方式</th>\n","      <th>抗凝劑</th>\n","      <th>...</th>\n","      <th>K (meq/l)</th>\n","      <th>離子鈣 (mg/dl)</th>\n","      <th>P (mg/dl)</th>\n","      <th>URR</th>\n","      <th>Kt/V (Gotch)</th>\n","      <th>Kt/V</th>\n","      <th>nPCR</th>\n","      <th>TACurea</th>\n","      <th>Cardiac/thoracic ratio (%)</th>\n","      <th>Ca X P (mg2/dL2)</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>5004995</td>\n","      <td>31/11/18</td>\n","      <td>2</td>\n","      <td>1996/8/28</td>\n","      <td>N</td>\n","      <td>N</td>\n","      <td>19-30</td>\n","      <td>20190130.0</td>\n","      <td>3.0</td>\n","      <td>0.0</td>\n","      <td>...</td>\n","      <td>4.8</td>\n","      <td>4.24</td>\n","      <td>4.1</td>\n","      <td>0.87</td>\n","      <td>2.02</td>\n","      <td>2.45</td>\n","      <td>1.65</td>\n","      <td>47.0</td>\n","      <td>0.53</td>\n","      <td>34.77</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>3983367</td>\n","      <td>35/2/15</td>\n","      <td>1</td>\n","      <td>1995/12/11</td>\n","      <td>N</td>\n","      <td>N</td>\n","      <td>13-19-0</td>\n","      <td>20190130.0</td>\n","      <td>3.0</td>\n","      <td>1.0</td>\n","      <td>...</td>\n","      <td>4.3</td>\n","      <td>4.93</td>\n","      <td>4.0</td>\n","      <td>0.75</td>\n","      <td>1.39</td>\n","      <td>1.68</td>\n","      <td>1.46</td>\n","      <td>52.5</td>\n","      <td>0.48</td>\n","      <td>39.44</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>400509</td>\n","      <td>27/12/18</td>\n","      <td>2</td>\n","      <td>1993/3/16</td>\n","      <td>N</td>\n","      <td>N</td>\n","      <td>12-0-25</td>\n","      <td>20190130.0</td>\n","      <td>3.0</td>\n","      <td>1.0</td>\n","      <td>...</td>\n","      <td>2.9</td>\n","      <td>3.97</td>\n","      <td>2.3</td>\n","      <td>0.71</td>\n","      <td>1.25</td>\n","      <td>1.50</td>\n","      <td>0.48</td>\n","      <td>13.5</td>\n","      <td>0.52</td>\n","      <td>18.26</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>3468683</td>\n","      <td>41/2/17</td>\n","      <td>2</td>\n","      <td>1995/7/30</td>\n","      <td>N</td>\n","      <td>N</td>\n","      <td>8-11-12-19-25-0</td>\n","      <td>20190130.0</td>\n","      <td>3.0</td>\n","      <td>2.0</td>\n","      <td>...</td>\n","      <td>4.7</td>\n","      <td>5.12</td>\n","      <td>5.8</td>\n","      <td>0.83</td>\n","      <td>1.76</td>\n","      <td>2.15</td>\n","      <td>1.75</td>\n","      <td>54.5</td>\n","      <td>0.42</td>\n","      <td>59.39</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>2419091</td>\n","      <td>42/12/20</td>\n","      <td>2</td>\n","      <td>1994/12/12</td>\n","      <td>N</td>\n","      <td>N</td>\n","      <td>10-22-30-33-13-19-5-7</td>\n","      <td>20190130.0</td>\n","      <td>3.0</td>\n","      <td>0.0</td>\n","      <td>...</td>\n","      <td>4.3</td>\n","      <td>5.09</td>\n","      <td>4.6</td>\n","      <td>0.77</td>\n","      <td>1.45</td>\n","      <td>1.74</td>\n","      <td>1.44</td>\n","      <td>50.0</td>\n","      <td>0.62</td>\n","      <td>46.83</td>\n","    </tr>\n","    <tr>\n","      <th>...</th>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","    </tr>\n","    <tr>\n","      <th>460</th>\n","      <td>11895105</td>\n","      <td>30/11/28</td>\n","      <td>1</td>\n","      <td>2018/12/6</td>\n","      <td>N</td>\n","      <td>N</td>\n","      <td>19-22-29-30</td>\n","      <td>20190129.0</td>\n","      <td>3.0</td>\n","      <td>0.0</td>\n","      <td>...</td>\n","      <td>3.6</td>\n","      <td>4.05</td>\n","      <td>5.7</td>\n","      <td>0.73</td>\n","      <td>1.32</td>\n","      <td>1.61</td>\n","      <td>0.85</td>\n","      <td>28.5</td>\n","      <td>NaN</td>\n","      <td>46.17</td>\n","    </tr>\n","    <tr>\n","      <th>461</th>\n","      <td>1690080</td>\n","      <td>28/1/24</td>\n","      <td>2</td>\n","      <td>2018/12/29</td>\n","      <td>N</td>\n","      <td>N</td>\n","      <td>NaN</td>\n","      <td>20190129.0</td>\n","      <td>3.0</td>\n","      <td>0.0</td>\n","      <td>...</td>\n","      <td>3.1</td>\n","      <td>3.98</td>\n","      <td>4.5</td>\n","      <td>0.72</td>\n","      <td>1.26</td>\n","      <td>1.42</td>\n","      <td>1.57</td>\n","      <td>61.0</td>\n","      <td>NaN</td>\n","      <td>35.82</td>\n","    </tr>\n","    <tr>\n","      <th>462</th>\n","      <td>13944660</td>\n","      <td>51/1/29</td>\n","      <td>1</td>\n","      <td>2019/1/5</td>\n","      <td>N</td>\n","      <td>N</td>\n","      <td>1-25-29-30</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>...</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","    </tr>\n","    <tr>\n","      <th>463</th>\n","      <td>3846692</td>\n","      <td>22/2/28</td>\n","      <td>1</td>\n","      <td>2019/1/22</td>\n","      <td>N</td>\n","      <td>N</td>\n","      <td>10-19-22-29-30</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>...</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","    </tr>\n","    <tr>\n","      <th>464</th>\n","      <td>21217704</td>\n","      <td>40/4/26</td>\n","      <td>1</td>\n","      <td>2016/7/1</td>\n","      <td>N</td>\n","      <td>N</td>\n","      <td>19-25</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>...</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","    </tr>\n","  </tbody>\n","</table>\n","<p>465 rows × 49 columns</p>\n","</div>\n","      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-3b5e6046-d83a-47d6-bd60-f0c5f552f968')\"\n","              title=\"Convert this dataframe to an interactive table.\"\n","              style=\"display:none;\">\n","        \n","  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n","       width=\"24px\">\n","    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n","    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n","  </svg>\n","      </button>\n","      \n","  <style>\n","    .colab-df-container {\n","      display:flex;\n","      flex-wrap:wrap;\n","      gap: 12px;\n","    }\n","\n","    .colab-df-convert {\n","      background-color: #E8F0FE;\n","      border: none;\n","      border-radius: 50%;\n","      cursor: pointer;\n","      display: none;\n","      fill: #1967D2;\n","      height: 32px;\n","      padding: 0 0 0 0;\n","      width: 32px;\n","    }\n","\n","    .colab-df-convert:hover {\n","      background-color: #E2EBFA;\n","      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n","      fill: #174EA6;\n","    }\n","\n","    [theme=dark] .colab-df-convert {\n","      background-color: #3B4455;\n","      fill: #D2E3FC;\n","    }\n","\n","    [theme=dark] .colab-df-convert:hover {\n","      background-color: #434B5C;\n","      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n","      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n","      fill: #FFFFFF;\n","    }\n","  </style>\n","\n","      <script>\n","        const buttonEl =\n","          document.querySelector('#df-3b5e6046-d83a-47d6-bd60-f0c5f552f968 button.colab-df-convert');\n","        buttonEl.style.display =\n","          google.colab.kernel.accessAllowed ? 'block' : 'none';\n","\n","        async function convertToInteractive(key) {\n","          const element = document.querySelector('#df-3b5e6046-d83a-47d6-bd60-f0c5f552f968');\n","          const dataTable =\n","            await google.colab.kernel.invokeFunction('convertToInteractive',\n","                                                     [key], {});\n","          if (!dataTable) return;\n","\n","          const docLinkHtml = 'Like what you see? Visit the ' +\n","            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n","            + ' to learn more about interactive tables.';\n","          element.innerHTML = '';\n","          dataTable['output_type'] = 'display_data';\n","          await google.colab.output.renderOutput(dataTable, element);\n","          const docLink = document.createElement('div');\n","          docLink.innerHTML = docLinkHtml;\n","          element.appendChild(docLink);\n","        }\n","      </script>\n","    </div>\n","  </div>\n","  "]},"metadata":{},"execution_count":29}]},{"cell_type":"code","source":["def age(date):\n","    date = date.split('/')\n","    age = 111-int(date[0])\n","    return age\n","def treat_year(date):\n","    date = date.split('/')\n","    year = 2019-int(date[0])\n","    return year"],"metadata":{"id":"GgBKal3WnUTN","executionInfo":{"status":"ok","timestamp":1660021342347,"user_tz":-480,"elapsed":9,"user":{"displayName":"洪睿甫","userId":"05593760976211923971"}}},"execution_count":3,"outputs":[]},{"cell_type":"code","source":["df = data.rename(columns = {'合併症':'comorbidities', '性別':'gender', '離子鈣 (mg/dl)':'Ca', '生日':'birthday', \n","    '透析前體重(kg)':'weight', '本次透析時間(min)':'duration', '本次透析前BUN (mg/dl)':'BUN', '兩次透析時間間隔 (min)':'time_interval'})\n","df['Age'] = df['birthday'].map(age)\n","df['treatment_year'] = df['開始治療日期'].map(treat_year)\n","# 合併症因為還不確定空值所以先暫時刪掉，Glucose因為空值太多所以先刪掉, Kt/V (Gotch)適用另一種公式算的先刪掉\n","df = df.drop(columns = ['本次透析後BUN (mg/dl)', '透析後體重(kg)', '透析器型號', 'PlanDate', 'birthday', '開始治療日期','Glucose[AC] (mg/dl)','Kt/V (Gotch)']) \n"],"metadata":{"id":"T6d5c86Qk3hn","executionInfo":{"status":"ok","timestamp":1660021342348,"user_tz":-480,"elapsed":7,"user":{"displayName":"洪睿甫","userId":"05593760976211923971"}}},"execution_count":4,"outputs":[]},{"cell_type":"code","source":["#target_col = [col for col in x.columns if '(' in col]\n","def trim_column(col,df):\n","  new_col = col.split(' (')[0]\n","  df = df.rename(columns = {col:new_col})\n","  return df\n","col_list = df.filter(like=' (').columns\n","\n","for i in col_list:\n","  df = trim_column(i,df)"],"metadata":{"id":"XfkUSGlW2dav","executionInfo":{"status":"ok","timestamp":1660021342348,"user_tz":-480,"elapsed":7,"user":{"displayName":"洪睿甫","userId":"05593760976211923971"}}},"execution_count":5,"outputs":[]},{"cell_type":"code","source":["df = df[df[df.columns[4:15]].isna().sum(axis=1) == 0] # drop parameter contain null value\n","df = df[df.isna().sum(axis=1)<10] # 病患特徵中有3個人缺值超過10個欄位，這三個人直接刪掉\n","df = df.drop(396) # drop kt/v is null"],"metadata":{"id":"V443izafbh8-","executionInfo":{"status":"ok","timestamp":1660021342348,"user_tz":-480,"elapsed":6,"user":{"displayName":"洪睿甫","userId":"05593760976211923971"}}},"execution_count":6,"outputs":[]},{"cell_type":"code","source":["def split_comorbidity(x):\n","  output = []\n","  x = x.split('-')\n","  for i in x:\n","    if i!='0':\n","      output.append(int(i))\n","  #x = [int(i) for i in x]\n","  return output\n","df['comorbidities'] = df['comorbidities'].map(split_comorbidity)"],"metadata":{"id":"MuENLUZJL6o-","executionInfo":{"status":"ok","timestamp":1660021342349,"user_tz":-480,"elapsed":7,"user":{"displayName":"洪睿甫","userId":"05593760976211923971"}}},"execution_count":7,"outputs":[]},{"cell_type":"code","source":["# 要補空值的欄位有 W.B.C. (x1000/ul), Platelet (x1000/ul), Cardiac/thoracic ratio (%) 都補mean\n","df['Cardiac/thoracic ratio'] = df['Cardiac/thoracic ratio'].fillna(round(df['Cardiac/thoracic ratio'].mean(),2))\n","df['W.B.C.'] = df['W.B.C.'].fillna(round(df['W.B.C.'].mean(),1))\n","df['Platelet'] = df['Platelet'].fillna(round(df['Platelet'].mean(),0))"],"metadata":{"id":"v7dZl39LnL_D","executionInfo":{"status":"ok","timestamp":1660021342349,"user_tz":-480,"elapsed":6,"user":{"displayName":"洪睿甫","userId":"05593760976211923971"}}},"execution_count":8,"outputs":[]},{"cell_type":"code","source":["# split x and y\n","df = df[df['Kt/V']>1.2]\n","df = df.reset_index(drop = True)  # drop掉每筆都一樣的column\n","df = df.drop(columns = ['time_interval','duration','鹼基','Kt/V'])\n","x = df[df.columns[1:5].tolist()+df.columns[20:].tolist()]\n","\n","\n","# x\n","# encoding gender, HBsAg, Anti-HCV\n","for i in ['HBsAg','Anti-HCV']:\n","  tempdf = pd.get_dummies(x[i])\n","  for j in tempdf.columns:\n","    new_col_name = i+'_'+str(j)\n","    tempdf = tempdf.rename(columns={j:new_col_name})\n","  x = pd.concat([x,tempdf],axis=1)\n","x['gender'] = x['gender'].apply(lambda x:0 if x == 1 else 1)\n","x = x.drop(columns = ['HBsAg','Anti-HCV'])\n","\n","# standardize\n","scaler = StandardScaler().fit(x[x.columns[2:-6]])\n","x_scaled = scaler.transform(x[x.columns[2:-6]])\n","x_scaled = pd.DataFrame(data = x_scaled,columns = x.columns[2:-6].tolist())\n","patient_state = pd.concat([x[x.columns[0]],x[x.columns[-6:]],x_scaled],axis=1)\n","#patient_state = torch.Tensor(patient_state.to_numpy())\n"],"metadata":{"id":"jiRjoxX7O3wF","executionInfo":{"status":"ok","timestamp":1660021343049,"user_tz":-480,"elapsed":3,"user":{"displayName":"洪睿甫","userId":"05593760976211923971"}}},"execution_count":9,"outputs":[]},{"cell_type":"code","source":["from sklearn.decomposition import PCA\n","\n","comorbidity_df = pd.DataFrame(data = np.zeros((223,33),dtype=int),columns = ['c'+str(i+1) for i in range(33)])\n","comorbidity = x[x.columns[1]]\n","\n","for index,row in enumerate(comorbidity):\n","  comorbidity_df.loc[index,['c'+str(i) for i in row]] = 1\n","\n","pca = PCA(n_components=10)\n","pca.fit(comorbidity_df)\n","com_pca = pca.transform(comorbidity_df)\n","com_pca_df = pd.DataFrame(data = com_pca,columns=['com_pca_'+str(i+1) for i in range(10)])\n","patient_state = pd.concat([x[x.columns[0]],x[x.columns[-6:]],x_scaled,com_pca_df],axis=1)\n","patient_state = torch.Tensor(patient_state.to_numpy())"],"metadata":{"id":"uxjCUsA7IP6E","executionInfo":{"status":"ok","timestamp":1660021344753,"user_tz":-480,"elapsed":2,"user":{"displayName":"洪睿甫","userId":"05593760976211923971"}}},"execution_count":10,"outputs":[]},{"cell_type":"code","source":["\n","import matplotlib.pyplot as plt\n","plt.rcParams[\"figure.figsize\"] = (12,6)\n","\n","fig, ax = plt.subplots()\n","xi = np.arange(1, 11, step=1)\n","y = np.cumsum(pca.explained_variance_ratio_)\n","\n","plt.ylim(0.0,1.1)\n","plt.plot(xi, y, marker='o', linestyle='--', color='b')\n","\n","plt.xlabel('Number of Components')\n","plt.xticks(np.arange(0, 11, step=1)) #change from 0-based array index to 1-based human-readable label\n","plt.ylabel('Cumulative variance (%)')\n","plt.title('The number of components needed to explain variance')\n","\n","plt.axhline(y=0.8, color='r', linestyle='-')\n","#plt.text(0.5, 0.85, '95% cut-off threshold', color = 'red', fontsize=16)\n","\n","ax.grid(axis='x')\n","plt.show()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":404},"id":"89qvRK_8IqPN","executionInfo":{"status":"ok","timestamp":1660008696104,"user_tz":-480,"elapsed":688,"user":{"displayName":"洪睿甫","userId":"05593760976211923971"}},"outputId":"98e8ccee-caf9-4539-b0cd-d4422b7ef118"},"execution_count":null,"outputs":[{"output_type":"display_data","data":{"text/plain":["<Figure size 864x432 with 1 Axes>"],"image/png":"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\n"},"metadata":{"needs_background":"light"}}]},{"cell_type":"code","source":["y_dict = {}\n","y = df[df.columns[5:15].tolist()]\n","for i in ['透析方式','抗凝劑','鉀離子濃度','維持劑量','初劑量','每次透析時間','透析器表面積','鈣離子濃度']:\n","  le = preprocessing.LabelEncoder()\n","  le.fit(y[i])\n","  temp = { i : v for i ,v in enumerate(list(le.classes_))}\n","  y_dict[i] = temp\n","  y[i] = le.transform(y[i])\n","\n","# y\n","# 預測的時候 除了血液流速和透析流速之外都當作是類別型來預測，只有血液流速和透析流速 output_dim = 1, 其他output_dim = num_class\n","parameter = {}\n","label_name = y.columns\n","# 暫時先用一個參數當作預測目標\n","for i in y.columns:\n","  parameter[i] = torch.Tensor(y[i].to_numpy())\n","unit_dict = { i : len(y[i].value_counts()) for i in y.columns}\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"XTSKhx9KZLQf","executionInfo":{"status":"ok","timestamp":1660021346878,"user_tz":-480,"elapsed":5,"user":{"displayName":"洪睿甫","userId":"05593760976211923971"}},"outputId":"be789b16-8515-42ba-f350-e914f35f5bb5"},"execution_count":11,"outputs":[{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:8: SettingWithCopyWarning: \n","A value is trying to be set on a copy of a slice from a DataFrame.\n","Try using .loc[row_indexer,col_indexer] = value instead\n","\n","See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n","  \n"]}]},{"cell_type":"code","source":["class MyDataset(Dataset):\n","    def __init__(self,x,y):\n","        self.x = x\n","        self.y = y\n","        \n","        \n","    def __getitem__(self, index):\n","        self.output = {}\n","        self.output['patient_state'] = self.x[index]\n","       \n","        for i in self.y.keys():\n","          self.output[i] = self.y[i][index]\n","\n","        return self.output\n","    \n","    def __len__(self):\n","        return len(self.x)\n","\n","dataset = MyDataset(patient_state,parameter)\n","dataloader = DataLoader(dataset, batch_size=4, num_workers=2)\n","next(iter(dataloader))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"4lQib_65YY4V","executionInfo":{"status":"ok","timestamp":1660021349445,"user_tz":-480,"elapsed":1427,"user":{"displayName":"洪睿甫","userId":"05593760976211923971"}},"outputId":"2d400c16-7741-41ee-ab4b-dd9de33f4928"},"execution_count":12,"outputs":[{"output_type":"execute_result","data":{"text/plain":["{'patient_state': tensor([[ 1.0000e+00,  1.0000e+00,  0.0000e+00,  0.0000e+00,  1.0000e+00,\n","           0.0000e+00,  0.0000e+00, -3.8166e-01,  2.4859e+00,  2.5522e+00,\n","           7.2429e-01,  1.3463e-01, -1.5598e+00,  7.3765e-01, -8.9495e-01,\n","          -3.1283e-02,  4.0742e-01, -3.4826e-01, -7.2731e-01,  2.6678e+00,\n","           1.4609e+00,  3.0419e-01,  2.7281e-01, -7.8762e-01,  8.4567e-01,\n","           1.8244e+00, -1.5861e-01, -2.6609e-02, -4.8677e-01,  2.2379e-01,\n","           6.1707e-02, -3.3614e-01, -5.4707e-01, -9.6112e-02,  3.3610e-03,\n","          -1.2367e-01],\n","         [ 0.0000e+00,  1.0000e+00,  0.0000e+00,  0.0000e+00,  1.0000e+00,\n","           0.0000e+00,  0.0000e+00, -8.7081e-01, -5.0627e-01, -5.8467e-01,\n","           2.7442e-01, -1.7090e+00, -4.3659e-01,  7.9174e-01, -1.8606e-01,\n","           9.2000e-01, -3.1826e-01,  1.2068e+00, -8.0793e-01, -1.3023e-02,\n","           7.8960e-01,  7.6679e-01, -5.2558e-01, -3.8727e-01,  5.2108e-01,\n","           1.9539e+00, -7.7268e-01, -1.3522e-01,  1.2535e-01, -3.8025e-02,\n","          -5.5418e-04, -9.7599e-02, -4.2686e-01, -8.7363e-02, -3.7083e-02,\n","          -8.7000e-02],\n","         [ 1.0000e+00,  1.0000e+00,  0.0000e+00,  0.0000e+00,  1.0000e+00,\n","           0.0000e+00,  0.0000e+00, -1.6534e+00,  1.0263e+00,  5.3566e-01,\n","          -4.8019e-01, -1.9315e+00, -5.0398e-01, -2.6161e+00, -1.8957e+00,\n","           1.2371e+00, -2.3501e+00, -9.5675e-01, -2.1783e+00, -9.0663e-01,\n","          -2.6727e+00, -2.5135e+00,  1.1314e-01, -2.2030e+00,  1.1703e+00,\n","           2.2129e+00, -8.7354e-01,  6.3477e-01,  1.6013e-01,  4.7111e-01,\n","          -5.1971e-01,  3.2809e-01,  4.3938e-01, -9.8276e-02, -2.3037e-01,\n","          -1.7601e-01],\n","         [ 1.0000e+00,  1.0000e+00,  0.0000e+00,  0.0000e+00,  1.0000e+00,\n","           0.0000e+00,  0.0000e+00,  1.5749e+00,  2.2352e-01, -1.3654e-01,\n","           4.0503e-01,  8.9753e-01, -6.8369e-01,  1.2786e+00, -6.0960e-02,\n","          -3.1283e-02,  2.6228e-01,  1.6350e+00,  6.4309e-01,  1.7742e+00,\n","           1.8142e+00,  9.3501e-01, -1.4836e+00,  1.3230e+00,  3.4205e-02,\n","           1.9539e+00, -7.5617e-01,  5.6130e-01,  1.0196e-01, -2.2989e-01,\n","          -7.2268e-01,  4.7732e-01, -2.6439e-01,  9.8287e-01, -8.3260e-02,\n","          -2.6515e-01]]),\n"," '初劑量': tensor([0., 4., 7., 4.]),\n"," '抗凝劑': tensor([0., 1., 1., 2.]),\n"," '每次透析時間': tensor([5., 5., 5., 5.]),\n"," '維持劑量': tensor([0., 5., 7., 0.]),\n"," '血液流速': tensor([250., 250., 220., 280.]),\n"," '透析器表面積': tensor([3., 4., 3., 4.]),\n"," '透析方式': tensor([2., 2., 2., 2.]),\n"," '透析液流速': tensor([509., 500., 456., 511.]),\n"," '鈣離子濃度': tensor([0., 1., 1., 0.]),\n"," '鉀離子濃度': tensor([1., 0., 1., 1.])}"]},"metadata":{},"execution_count":12}]},{"cell_type":"code","source":["'''class Embedding(keras.layers.Layer):\n","  def __init__(self, input_dim, output_dim):\n","    super(Embedding, self).__init__()\n","    self.embed_layer = tf.keras.layers.Embedding(input_dim, output_dim)#,input_length=\n","  \n","  def call(self, input):\n","    output = self.embed_layer(input)\n","    return output'''"],"metadata":{"id":"hSiBPo0lO5zI","colab":{"base_uri":"https://localhost:8080/","height":53},"executionInfo":{"status":"ok","timestamp":1659977639824,"user_tz":-480,"elapsed":467,"user":{"displayName":"洪睿甫","userId":"05593760976211923971"}},"outputId":"727b9088-6468-4f4b-aa41-caa0a5fa38d4"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["'class Embedding(keras.layers.Layer):\\n  def __init__(self, input_dim, output_dim):\\n    super(Embedding, self).__init__()\\n    self.embed_layer = tf.keras.layers.Embedding(input_dim, output_dim)#,input_length=\\n  \\n  def call(self, input):\\n    output = self.embed_layer(input)\\n    return output'"],"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"}},"metadata":{},"execution_count":16}]},{"cell_type":"code","source":["class Net(nn.Module):\n","  def __init__(self, unit_dict):\n","    super(Net, self).__init__()\n","    self.layer1 = nn.Linear(36, 64) \n","    self.relu = nn.ReLU()\n","    self.layer2 = nn.Linear(64, 32)\n","    self.layer3 = nn.Linear(32, 16)\n","    self.branch1 = nn.Linear(16, unit_dict['透析方式'])\n","    self.branch2 = nn.Linear(16, unit_dict['抗凝劑'])\n","    self.branch3 = nn.Linear(16, unit_dict['初劑量'])\n","    self.branch4 = nn.Linear(16, unit_dict['維持劑量'])\n","    self.branch5 = nn.Linear(16, 1)\n","    self.branch6 = nn.Linear(16, 1)\n","    self.branch7 = nn.Linear(16, unit_dict['每次透析時間'])\n","    self.branch8 = nn.Linear(16, unit_dict['透析器表面積'])\n","    self.branch9 = nn.Linear(16, unit_dict['鈣離子濃度'])\n","    self.branch10 = nn.Linear(16, unit_dict['鉀離子濃度'])\n","\n","  def forward(self, x):\n","    x = self.layer1(x)\n","    x = self.relu(x)\n","    x = self.layer2(x)\n","    x = self.relu(x)\n","    x = self.layer3(x)\n","    label1 = self.branch1(x)\n","    label2 = self.branch2(x)\n","    label3 = self.branch3(x)\n","    label4 = self.branch4(x)\n","    label5 = self.branch5(x)\n","    label6 = self.branch6(x)\n","    label7 = self.branch7(x)\n","    label8 = self.branch8(x)\n","    label9 = self.branch9(x)\n","    label10 = self.branch10(x)\n","\n","    return {'透析方式' : label1, '抗凝劑' : label2, '初劑量' : label3, '維持劑量' : label4, '血液流速' : label5, \n","          '透析液流速' : label6, '每次透析時間' : label7, '透析器表面積' : label8, '鈣離子濃度' : label9, '鉀離子濃度' : label10}\n","model = Net(unit_dict)\n","model_uncertainty = Net(unit_dict)"],"metadata":{"id":"MODSciJ0EJA3","executionInfo":{"status":"ok","timestamp":1660021351448,"user_tz":-480,"elapsed":2,"user":{"displayName":"洪睿甫","userId":"05593760976211923971"}}},"execution_count":13,"outputs":[]},{"cell_type":"code","source":["print(model)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"n1kLw6k4SxqM","executionInfo":{"status":"ok","timestamp":1660009846864,"user_tz":-480,"elapsed":285,"user":{"displayName":"洪睿甫","userId":"05593760976211923971"}},"outputId":"51503f39-dc53-45f9-b462-db2bc66a5f4f"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Net(\n","  (layer1): Linear(in_features=36, out_features=64, bias=True)\n","  (relu): ReLU()\n","  (layer2): Linear(in_features=64, out_features=32, bias=True)\n","  (layer3): Linear(in_features=32, out_features=16, bias=True)\n","  (branch1): Linear(in_features=16, out_features=4, bias=True)\n","  (branch2): Linear(in_features=16, out_features=3, bias=True)\n","  (branch3): Linear(in_features=16, out_features=10, bias=True)\n","  (branch4): Linear(in_features=16, out_features=10, bias=True)\n","  (branch5): Linear(in_features=16, out_features=1, bias=True)\n","  (branch6): Linear(in_features=16, out_features=1, bias=True)\n","  (branch7): Linear(in_features=16, out_features=6, bias=True)\n","  (branch8): Linear(in_features=16, out_features=7, bias=True)\n","  (branch9): Linear(in_features=16, out_features=3, bias=True)\n","  (branch10): Linear(in_features=16, out_features=2, bias=True)\n",")\n"]}]},{"cell_type":"code","source":["[p for p in model.parameters()][0],[p for p in model_uncertainty.parameters()][0]"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"PktI2je5Vpg-","executionInfo":{"status":"ok","timestamp":1659977645162,"user_tz":-480,"elapsed":372,"user":{"displayName":"洪睿甫","userId":"05593760976211923971"}},"outputId":"e787ee0c-9ca6-413a-cb0b-2bf10a2cf777"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["(Parameter containing:\n"," tensor([[-0.1372, -0.1227, -0.0642,  ..., -0.1561, -0.0973,  0.1433],\n","         [ 0.0744,  0.0808,  0.0088,  ..., -0.0611,  0.0655,  0.1381],\n","         [ 0.1450,  0.1471,  0.0332,  ..., -0.1286, -0.0092,  0.0250],\n","         ...,\n","         [-0.0252, -0.0945,  0.0877,  ..., -0.0229,  0.0493, -0.0309],\n","         [-0.1663,  0.1172,  0.1651,  ...,  0.1019, -0.0240, -0.0947],\n","         [ 0.1333,  0.0775,  0.0475,  ...,  0.1016, -0.1579, -0.1493]],\n","        requires_grad=True), Parameter containing:\n"," tensor([[-0.1476,  0.0467,  0.0112,  ..., -0.1041,  0.0288, -0.0480],\n","         [ 0.1398, -0.0249, -0.1217,  ...,  0.1665,  0.1464,  0.0958],\n","         [ 0.0857,  0.0660, -0.0292,  ..., -0.0463, -0.0213, -0.0460],\n","         ...,\n","         [ 0.1165, -0.0849, -0.1357,  ...,  0.0088, -0.0474,  0.0411],\n","         [ 0.1318, -0.1529,  0.1658,  ..., -0.1593,  0.0316,  0.1208],\n","         [ 0.1477, -0.1357,  0.1287,  ...,  0.1129, -0.0271, -0.1483]],\n","        requires_grad=True))"]},"metadata":{},"execution_count":18}]},{"cell_type":"code","source":["criterion1 = nn.CrossEntropyLoss()\n","criterion2 = nn.MSELoss()\n","criterion3 = nn.L1Loss()\n","loss_func = {'categorical' : criterion1 ,'continuous' : criterion2, 'continuous1' : criterion3}\n","log_var_dict = {i : torch.zeros((1,), requires_grad=True) for i in y.columns}\n","\n","params = ([p for p in model_uncertainty.parameters()])\n","params += [ i for i in log_var_dict.values()]\n","optimizer = torch.optim.Adam(model.parameters(), lr=0.01, betas=(0.9, 0.999), eps=1e-08, weight_decay=0, amsgrad=False)\n","optimizer_uncertainty = torch.optim.Adam(params, lr=0.01, betas=(0.9, 0.999), eps=1e-08, weight_decay=0, amsgrad=False)\n","print(torch.exp(log_var_dict['透析方式'])**0.5, torch.exp(log_var_dict['抗凝劑'])**0.5)"],"metadata":{"id":"s8mky4-WQ3ab","executionInfo":{"status":"ok","timestamp":1660021358954,"user_tz":-480,"elapsed":493,"user":{"displayName":"洪睿甫","userId":"05593760976211923971"}},"colab":{"base_uri":"https://localhost:8080/"},"outputId":"7c705246-e407-49ca-995f-bcb6bc94e958"},"execution_count":14,"outputs":[{"output_type":"stream","name":"stdout","text":["tensor([1.], grad_fn=<PowBackward0>) tensor([1.], grad_fn=<PowBackward0>)\n"]}]},{"cell_type":"code","source":["def criterion(y_pred, y_true, loss_func, continuous):\n","  loss = 0\n","  if continuous == True:\n","    loss_value = loss_func(y_pred,y_true)\n","  else:\n","    loss_value = loss_func(y_pred,y_true.squeeze().type(torch.LongTensor))\n","  return loss_value\n","\n","def criterion_uncertainty(y_pred, y_true, log_vars, loss_func, continuous):\n","  loss = 0\n","  precision = torch.exp(-log_vars)\n","  if continuous == True:\n","    loss_value = loss_func(y_pred,y_true)\n","  else:\n","    loss_value = loss_func(y_pred,y_true.squeeze().type(torch.LongTensor))\n","\n","  loss = torch.sum(precision * loss_value + log_vars, -1)\n","  return loss"],"metadata":{"id":"nu4eMh81f4gr","executionInfo":{"status":"ok","timestamp":1660021360920,"user_tz":-480,"elapsed":3,"user":{"displayName":"洪睿甫","userId":"05593760976211923971"}}},"execution_count":15,"outputs":[]},{"cell_type":"code","source":["loss_list1 = []\n","# baseline\n","for epoch in range(100):\n","    train_loss = 0.0\n","    # train the model #\n","    model.train()\n","    for index, dataset in enumerate(dataloader):\n","        loss = 0\n","        patient_state = dataset['patient_state']\n","        # zero the parameter gradients\n","        optimizer.zero_grad()\n","        output=model(patient_state)\n","        \n","        for i in list(dataset.keys())[1:]:\n","          label = dataset[i]\n","          label_hat = output[i]\n","          #log_var = log_var_dict[i]\n","          if i == '血液流速' or i == '透析液流速':\n","            #label_hat = label_hat.squeeze()\n","            loss += criterion(label_hat,label,loss_func['continuous'],True)\n","          else:\n","            loss += criterion(label_hat,label,loss_func['categorical'],False)\n","        loss = torch.mean(loss)\n","        \n","    \n","        # back prop\n","        loss.backward() \n","        # grad\n","        optimizer.step()\n","        #train_loss = train_loss + ((1 / (index + 1)) * (loss.data - train_loss))\n","        if index % 20 == 0:\n","            loss_list1.append(loss.item())\n","            print('Epoch %d, Batch %d trainloss: %.6f' %\n","              (epoch, index + 1, loss))\n","    "],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"vqRxn0A0lVoD","executionInfo":{"status":"ok","timestamp":1660021432251,"user_tz":-480,"elapsed":69453,"user":{"displayName":"洪睿甫","userId":"05593760976211923971"}},"outputId":"08e692c8-9360-4950-ef05-e94d71e7c6b4"},"execution_count":16,"outputs":[{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/torch/nn/modules/loss.py:530: UserWarning: Using a target size (torch.Size([4])) that is different to the input size (torch.Size([4, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n","  return F.mse_loss(input, target, reduction=self.reduction)\n"]},{"output_type":"stream","name":"stdout","text":["Epoch 0, Batch 1 trainloss: 307067.468750\n","Epoch 0, Batch 21 trainloss: 225223.312500\n","Epoch 0, Batch 41 trainloss: 40888.335938\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/torch/nn/modules/loss.py:530: UserWarning: Using a target size (torch.Size([3])) that is different to the input size (torch.Size([3, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n","  return F.mse_loss(input, target, reduction=self.reduction)\n"]},{"output_type":"stream","name":"stdout","text":["Epoch 1, Batch 1 trainloss: 143086.546875\n","Epoch 1, Batch 21 trainloss: 7476.962402\n","Epoch 1, Batch 41 trainloss: 18556.773438\n","Epoch 2, Batch 1 trainloss: 62971.132812\n","Epoch 2, Batch 21 trainloss: 4161.325195\n","Epoch 2, Batch 41 trainloss: 7563.540527\n","Epoch 3, Batch 1 trainloss: 24281.513672\n","Epoch 3, Batch 21 trainloss: 3461.601562\n","Epoch 3, Batch 41 trainloss: 5031.646973\n","Epoch 4, Batch 1 trainloss: 10178.065430\n","Epoch 4, Batch 21 trainloss: 2956.205566\n","Epoch 4, Batch 41 trainloss: 3868.209717\n","Epoch 5, Batch 1 trainloss: 5377.435547\n","Epoch 5, Batch 21 trainloss: 2801.317627\n","Epoch 5, Batch 41 trainloss: 3325.409180\n","Epoch 6, Batch 1 trainloss: 3410.627686\n","Epoch 6, Batch 21 trainloss: 2679.203613\n","Epoch 6, Batch 41 trainloss: 3057.757324\n","Epoch 7, Batch 1 trainloss: 2573.331299\n","Epoch 7, Batch 21 trainloss: 2615.528076\n","Epoch 7, Batch 41 trainloss: 2963.184326\n","Epoch 8, Batch 1 trainloss: 2145.748779\n","Epoch 8, Batch 21 trainloss: 2562.126953\n","Epoch 8, Batch 41 trainloss: 2856.154297\n","Epoch 9, Batch 1 trainloss: 1891.168945\n","Epoch 9, Batch 21 trainloss: 2528.410156\n","Epoch 9, Batch 41 trainloss: 2830.385986\n","Epoch 10, Batch 1 trainloss: 1680.234375\n","Epoch 10, Batch 21 trainloss: 2504.957275\n","Epoch 10, Batch 41 trainloss: 2783.057617\n","Epoch 11, Batch 1 trainloss: 1613.883545\n","Epoch 11, Batch 21 trainloss: 2465.984863\n","Epoch 11, Batch 41 trainloss: 2752.408691\n","Epoch 12, Batch 1 trainloss: 1513.154053\n","Epoch 12, Batch 21 trainloss: 2442.371094\n","Epoch 12, Batch 41 trainloss: 2761.896973\n","Epoch 13, Batch 1 trainloss: 1448.694946\n","Epoch 13, Batch 21 trainloss: 2431.791992\n","Epoch 13, Batch 41 trainloss: 2714.239258\n","Epoch 14, Batch 1 trainloss: 1399.252563\n","Epoch 14, Batch 21 trainloss: 2407.552246\n","Epoch 14, Batch 41 trainloss: 2713.812012\n","Epoch 15, Batch 1 trainloss: 1351.827515\n","Epoch 15, Batch 21 trainloss: 2388.143311\n","Epoch 15, Batch 41 trainloss: 2706.050781\n","Epoch 16, Batch 1 trainloss: 1307.757812\n","Epoch 16, Batch 21 trainloss: 2375.444580\n","Epoch 16, Batch 41 trainloss: 2682.030029\n","Epoch 17, Batch 1 trainloss: 1295.015381\n","Epoch 17, Batch 21 trainloss: 2363.470703\n","Epoch 17, Batch 41 trainloss: 2716.422119\n","Epoch 18, Batch 1 trainloss: 1278.683350\n","Epoch 18, Batch 21 trainloss: 2355.783203\n","Epoch 18, Batch 41 trainloss: 2669.169922\n","Epoch 19, Batch 1 trainloss: 1297.973145\n","Epoch 19, Batch 21 trainloss: 2360.293945\n","Epoch 19, Batch 41 trainloss: 2695.554199\n","Epoch 20, Batch 1 trainloss: 1325.098389\n","Epoch 20, Batch 21 trainloss: 2388.826172\n","Epoch 20, Batch 41 trainloss: 2619.231201\n","Epoch 21, Batch 1 trainloss: 1386.262329\n","Epoch 21, Batch 21 trainloss: 2400.342529\n","Epoch 21, Batch 41 trainloss: 2651.450928\n","Epoch 22, Batch 1 trainloss: 1450.503052\n","Epoch 22, Batch 21 trainloss: 2409.335938\n","Epoch 22, Batch 41 trainloss: 2692.507324\n","Epoch 23, Batch 1 trainloss: 1563.555054\n","Epoch 23, Batch 21 trainloss: 2432.645752\n","Epoch 23, Batch 41 trainloss: 2628.713135\n","Epoch 24, Batch 1 trainloss: 1799.601807\n","Epoch 24, Batch 21 trainloss: 2450.281250\n","Epoch 24, Batch 41 trainloss: 2754.592773\n","Epoch 25, Batch 1 trainloss: 1785.936768\n","Epoch 25, Batch 21 trainloss: 2468.024414\n","Epoch 25, Batch 41 trainloss: 2543.274902\n","Epoch 26, Batch 1 trainloss: 2621.670166\n","Epoch 26, Batch 21 trainloss: 2481.181885\n","Epoch 26, Batch 41 trainloss: 2938.101074\n","Epoch 27, Batch 1 trainloss: 2210.696777\n","Epoch 27, Batch 21 trainloss: 2469.298828\n","Epoch 27, Batch 41 trainloss: 2862.468018\n","Epoch 28, Batch 1 trainloss: 2835.753174\n","Epoch 28, Batch 21 trainloss: 2702.978027\n","Epoch 28, Batch 41 trainloss: 2617.063965\n","Epoch 29, Batch 1 trainloss: 4159.910156\n","Epoch 29, Batch 21 trainloss: 2701.101562\n","Epoch 29, Batch 41 trainloss: 4679.847168\n","Epoch 30, Batch 1 trainloss: 4848.327637\n","Epoch 30, Batch 21 trainloss: 2682.340820\n","Epoch 30, Batch 41 trainloss: 3286.804199\n","Epoch 31, Batch 1 trainloss: 4526.378906\n","Epoch 31, Batch 21 trainloss: 3505.631836\n","Epoch 31, Batch 41 trainloss: 6302.519531\n","Epoch 32, Batch 1 trainloss: 16292.404297\n","Epoch 32, Batch 21 trainloss: 3223.003662\n","Epoch 32, Batch 41 trainloss: 5991.894043\n","Epoch 33, Batch 1 trainloss: 13875.998047\n","Epoch 33, Batch 21 trainloss: 3291.090332\n","Epoch 33, Batch 41 trainloss: 8131.845215\n","Epoch 34, Batch 1 trainloss: 38778.371094\n","Epoch 34, Batch 21 trainloss: 6622.471680\n","Epoch 34, Batch 41 trainloss: 12175.802734\n","Epoch 35, Batch 1 trainloss: 34382.609375\n","Epoch 35, Batch 21 trainloss: 5860.629883\n","Epoch 35, Batch 41 trainloss: 6243.826172\n","Epoch 36, Batch 1 trainloss: 6091.331543\n","Epoch 36, Batch 21 trainloss: 7715.835938\n","Epoch 36, Batch 41 trainloss: 4677.743164\n","Epoch 37, Batch 1 trainloss: 5634.186035\n","Epoch 37, Batch 21 trainloss: 4323.059570\n","Epoch 37, Batch 41 trainloss: 4457.916016\n","Epoch 38, Batch 1 trainloss: 4442.366699\n","Epoch 38, Batch 21 trainloss: 7020.455566\n","Epoch 38, Batch 41 trainloss: 6339.650879\n","Epoch 39, Batch 1 trainloss: 8774.535156\n","Epoch 39, Batch 21 trainloss: 9328.794922\n","Epoch 39, Batch 41 trainloss: 4325.183105\n","Epoch 40, Batch 1 trainloss: 10748.335938\n","Epoch 40, Batch 21 trainloss: 5009.780762\n","Epoch 40, Batch 41 trainloss: 2085.372559\n","Epoch 41, Batch 1 trainloss: 7497.805664\n","Epoch 41, Batch 21 trainloss: 2892.682373\n","Epoch 41, Batch 41 trainloss: 2419.242188\n","Epoch 42, Batch 1 trainloss: 4047.329346\n","Epoch 42, Batch 21 trainloss: 2541.727051\n","Epoch 42, Batch 41 trainloss: 2734.533691\n","Epoch 43, Batch 1 trainloss: 2299.654541\n","Epoch 43, Batch 21 trainloss: 2617.149170\n","Epoch 43, Batch 41 trainloss: 2677.181641\n","Epoch 44, Batch 1 trainloss: 1599.564697\n","Epoch 44, Batch 21 trainloss: 2676.860107\n","Epoch 44, Batch 41 trainloss: 2580.190674\n","Epoch 45, Batch 1 trainloss: 1347.698730\n","Epoch 45, Batch 21 trainloss: 2668.616211\n","Epoch 45, Batch 41 trainloss: 2472.684814\n","Epoch 46, Batch 1 trainloss: 1269.121704\n","Epoch 46, Batch 21 trainloss: 2648.364014\n","Epoch 46, Batch 41 trainloss: 2409.038330\n","Epoch 47, Batch 1 trainloss: 1200.988770\n","Epoch 47, Batch 21 trainloss: 2611.589600\n","Epoch 47, Batch 41 trainloss: 2337.042480\n","Epoch 48, Batch 1 trainloss: 1172.101562\n","Epoch 48, Batch 21 trainloss: 2565.758545\n","Epoch 48, Batch 41 trainloss: 2283.535889\n","Epoch 49, Batch 1 trainloss: 1144.634644\n","Epoch 49, Batch 21 trainloss: 2528.473877\n","Epoch 49, Batch 41 trainloss: 2225.245605\n","Epoch 50, Batch 1 trainloss: 1144.347534\n","Epoch 50, Batch 21 trainloss: 2498.290527\n","Epoch 50, Batch 41 trainloss: 2168.183105\n","Epoch 51, Batch 1 trainloss: 1135.743774\n","Epoch 51, Batch 21 trainloss: 2477.549561\n","Epoch 51, Batch 41 trainloss: 2121.447754\n","Epoch 52, Batch 1 trainloss: 1153.239502\n","Epoch 52, Batch 21 trainloss: 2453.410156\n","Epoch 52, Batch 41 trainloss: 2089.544434\n","Epoch 53, Batch 1 trainloss: 1160.662842\n","Epoch 53, Batch 21 trainloss: 2454.771240\n","Epoch 53, Batch 41 trainloss: 2013.482178\n","Epoch 54, Batch 1 trainloss: 1172.526245\n","Epoch 54, Batch 21 trainloss: 2452.448486\n","Epoch 54, Batch 41 trainloss: 1999.457520\n","Epoch 55, Batch 1 trainloss: 1181.820190\n","Epoch 55, Batch 21 trainloss: 2435.893799\n","Epoch 55, Batch 41 trainloss: 1905.029175\n","Epoch 56, Batch 1 trainloss: 1185.457275\n","Epoch 56, Batch 21 trainloss: 2434.604004\n","Epoch 56, Batch 41 trainloss: 1907.090820\n","Epoch 57, Batch 1 trainloss: 1167.930298\n","Epoch 57, Batch 21 trainloss: 2422.369141\n","Epoch 57, Batch 41 trainloss: 1849.611816\n","Epoch 58, Batch 1 trainloss: 1149.242920\n","Epoch 58, Batch 21 trainloss: 2396.665527\n","Epoch 58, Batch 41 trainloss: 1827.398315\n","Epoch 59, Batch 1 trainloss: 1121.202881\n","Epoch 59, Batch 21 trainloss: 2373.771240\n","Epoch 59, Batch 41 trainloss: 1812.668213\n","Epoch 60, Batch 1 trainloss: 1102.535278\n","Epoch 60, Batch 21 trainloss: 2352.950439\n","Epoch 60, Batch 41 trainloss: 1822.256592\n","Epoch 61, Batch 1 trainloss: 1100.596680\n","Epoch 61, Batch 21 trainloss: 2363.860840\n","Epoch 61, Batch 41 trainloss: 1776.229614\n","Epoch 62, Batch 1 trainloss: 1126.121826\n","Epoch 62, Batch 21 trainloss: 2310.022217\n","Epoch 62, Batch 41 trainloss: 1817.290894\n","Epoch 63, Batch 1 trainloss: 1154.894409\n","Epoch 63, Batch 21 trainloss: 2292.169678\n","Epoch 63, Batch 41 trainloss: 1799.810303\n","Epoch 64, Batch 1 trainloss: 1242.976807\n","Epoch 64, Batch 21 trainloss: 2278.349121\n","Epoch 64, Batch 41 trainloss: 1815.385254\n","Epoch 65, Batch 1 trainloss: 1412.392578\n","Epoch 65, Batch 21 trainloss: 2272.379150\n","Epoch 65, Batch 41 trainloss: 1864.075317\n","Epoch 66, Batch 1 trainloss: 1656.965332\n","Epoch 66, Batch 21 trainloss: 2242.647461\n","Epoch 66, Batch 41 trainloss: 1902.078125\n","Epoch 67, Batch 1 trainloss: 1972.073975\n","Epoch 67, Batch 21 trainloss: 2241.409424\n","Epoch 67, Batch 41 trainloss: 1909.802490\n","Epoch 68, Batch 1 trainloss: 2494.006348\n","Epoch 68, Batch 21 trainloss: 2247.179443\n","Epoch 68, Batch 41 trainloss: 1938.500000\n","Epoch 69, Batch 1 trainloss: 3175.973877\n","Epoch 69, Batch 21 trainloss: 2253.782959\n","Epoch 69, Batch 41 trainloss: 2147.059082\n","Epoch 70, Batch 1 trainloss: 4315.316895\n","Epoch 70, Batch 21 trainloss: 2334.313232\n","Epoch 70, Batch 41 trainloss: 2247.043457\n","Epoch 71, Batch 1 trainloss: 6169.565918\n","Epoch 71, Batch 21 trainloss: 2422.942139\n","Epoch 71, Batch 41 trainloss: 2982.437256\n","Epoch 72, Batch 1 trainloss: 9148.512695\n","Epoch 72, Batch 21 trainloss: 2795.289551\n","Epoch 72, Batch 41 trainloss: 3467.589844\n","Epoch 73, Batch 1 trainloss: 13333.559570\n","Epoch 73, Batch 21 trainloss: 3993.171143\n","Epoch 73, Batch 41 trainloss: 5153.241211\n","Epoch 74, Batch 1 trainloss: 32623.671875\n","Epoch 74, Batch 21 trainloss: 8585.389648\n","Epoch 74, Batch 41 trainloss: 6290.443359\n","Epoch 75, Batch 1 trainloss: 50537.082031\n","Epoch 75, Batch 21 trainloss: 11036.724609\n","Epoch 75, Batch 41 trainloss: 14494.258789\n","Epoch 76, Batch 1 trainloss: 30714.365234\n","Epoch 76, Batch 21 trainloss: 7798.419922\n","Epoch 76, Batch 41 trainloss: 10483.943359\n","Epoch 77, Batch 1 trainloss: 4988.806641\n","Epoch 77, Batch 21 trainloss: 16800.150391\n","Epoch 77, Batch 41 trainloss: 4813.062988\n","Epoch 78, Batch 1 trainloss: 15847.732422\n","Epoch 78, Batch 21 trainloss: 14186.260742\n","Epoch 78, Batch 41 trainloss: 4232.965820\n","Epoch 79, Batch 1 trainloss: 10878.026367\n","Epoch 79, Batch 21 trainloss: 12771.962891\n","Epoch 79, Batch 41 trainloss: 3558.571533\n","Epoch 80, Batch 1 trainloss: 1882.738037\n","Epoch 80, Batch 21 trainloss: 4736.804199\n","Epoch 80, Batch 41 trainloss: 2997.787598\n","Epoch 81, Batch 1 trainloss: 3683.129395\n","Epoch 81, Batch 21 trainloss: 2950.493896\n","Epoch 81, Batch 41 trainloss: 3274.077637\n","Epoch 82, Batch 1 trainloss: 5098.263184\n","Epoch 82, Batch 21 trainloss: 3809.028564\n","Epoch 82, Batch 41 trainloss: 2844.287598\n","Epoch 83, Batch 1 trainloss: 2255.395508\n","Epoch 83, Batch 21 trainloss: 3480.828369\n","Epoch 83, Batch 41 trainloss: 2040.267700\n","Epoch 84, Batch 1 trainloss: 2498.438721\n","Epoch 84, Batch 21 trainloss: 2888.523926\n","Epoch 84, Batch 41 trainloss: 1931.497070\n","Epoch 85, Batch 1 trainloss: 8944.595703\n","Epoch 85, Batch 21 trainloss: 3841.719482\n","Epoch 85, Batch 41 trainloss: 2096.335938\n","Epoch 86, Batch 1 trainloss: 16450.412109\n","Epoch 86, Batch 21 trainloss: 5370.429688\n","Epoch 86, Batch 41 trainloss: 2120.347168\n","Epoch 87, Batch 1 trainloss: 18752.861328\n","Epoch 87, Batch 21 trainloss: 5723.788574\n","Epoch 87, Batch 41 trainloss: 2045.072021\n","Epoch 88, Batch 1 trainloss: 13234.223633\n","Epoch 88, Batch 21 trainloss: 4521.070801\n","Epoch 88, Batch 41 trainloss: 1890.869019\n","Epoch 89, Batch 1 trainloss: 8642.618164\n","Epoch 89, Batch 21 trainloss: 3333.222900\n","Epoch 89, Batch 41 trainloss: 1403.868408\n","Epoch 90, Batch 1 trainloss: 4742.745605\n","Epoch 90, Batch 21 trainloss: 3564.767578\n","Epoch 90, Batch 41 trainloss: 1482.713623\n","Epoch 91, Batch 1 trainloss: 5641.238770\n","Epoch 91, Batch 21 trainloss: 3315.887451\n","Epoch 91, Batch 41 trainloss: 1454.393555\n","Epoch 92, Batch 1 trainloss: 5196.238281\n","Epoch 92, Batch 21 trainloss: 3502.886475\n","Epoch 92, Batch 41 trainloss: 1484.026489\n","Epoch 93, Batch 1 trainloss: 5527.285156\n","Epoch 93, Batch 21 trainloss: 3684.525391\n","Epoch 93, Batch 41 trainloss: 1523.307861\n","Epoch 94, Batch 1 trainloss: 7022.611816\n","Epoch 94, Batch 21 trainloss: 4225.952637\n","Epoch 94, Batch 41 trainloss: 1628.236572\n","Epoch 95, Batch 1 trainloss: 7685.652344\n","Epoch 95, Batch 21 trainloss: 4549.058105\n","Epoch 95, Batch 41 trainloss: 1661.528564\n","Epoch 96, Batch 1 trainloss: 8370.401367\n","Epoch 96, Batch 21 trainloss: 4983.070312\n","Epoch 96, Batch 41 trainloss: 1644.606567\n","Epoch 97, Batch 1 trainloss: 8561.350586\n","Epoch 97, Batch 21 trainloss: 5883.578125\n","Epoch 97, Batch 41 trainloss: 2109.496582\n","Epoch 98, Batch 1 trainloss: 7229.735840\n","Epoch 98, Batch 21 trainloss: 6043.732422\n","Epoch 98, Batch 41 trainloss: 1891.600464\n","Epoch 99, Batch 1 trainloss: 6170.667969\n","Epoch 99, Batch 21 trainloss: 8834.040039\n","Epoch 99, Batch 41 trainloss: 2494.294922\n"]}]},{"cell_type":"code","source":["loss_list2 = []\n","# uncertainty\n","for epoch in range(100):\n","    train_loss = 0.0\n","    # train the model #\n","    model_uncertainty.train()\n","    for index, dataset in enumerate(dataloader):\n","        loss = 0\n","        patient_state = dataset['patient_state']\n","        # zero the parameter gradients\n","        optimizer_uncertainty.zero_grad()\n","        output=model_uncertainty(patient_state)\n","        \n","        for i in list(dataset.keys())[1:]:\n","          label = dataset[i]\n","          label_hat = output[i]\n","          log_var = log_var_dict[i]\n","          if i == '血液流速' or i == '透析液流速':\n","            #label_hat = label_hat.squeeze()\n","            loss += criterion_uncertainty(label_hat,label,log_var,loss_func['continuous'],True)\n","          else:\n","            loss += criterion_uncertainty(label_hat,label,log_var,loss_func['categorical'],False)\n","\n","        loss = torch.mean(loss)\n","        \n","        # back prop\n","        loss.backward() \n","        # grad\n","        optimizer_uncertainty.step()\n","        #train_loss = train_loss + ((1 / (index + 1)) * (loss.data - train_loss))\n","        if index % 20 == 0:\n","            loss_list2.append(loss.item())\n","            print('Epoch %d, Batch %d trainloss: %.6f' %\n","              (epoch, index + 1, loss))\n","        \n"],"metadata":{"id":"C8jH6vCgUwnM","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1660021507980,"user_tz":-480,"elapsed":75748,"user":{"displayName":"洪睿甫","userId":"05593760976211923971"}},"outputId":"e4e794b3-094c-4371-c242-df65da89f006"},"execution_count":17,"outputs":[{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/torch/nn/modules/loss.py:530: UserWarning: Using a target size (torch.Size([4])) that is different to the input size (torch.Size([4, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n","  return F.mse_loss(input, target, reduction=self.reduction)\n"]},{"output_type":"stream","name":"stdout","text":["Epoch 0, Batch 1 trainloss: 307427.531250\n","Epoch 0, Batch 21 trainloss: 220356.859375\n","Epoch 0, Batch 41 trainloss: 29861.261719\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/torch/nn/modules/loss.py:530: UserWarning: Using a target size (torch.Size([3])) that is different to the input size (torch.Size([3, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n","  return F.mse_loss(input, target, reduction=self.reduction)\n"]},{"output_type":"stream","name":"stdout","text":["Epoch 1, Batch 1 trainloss: 89523.859375\n","Epoch 1, Batch 21 trainloss: 3561.317627\n","Epoch 1, Batch 41 trainloss: 10534.440430\n","Epoch 2, Batch 1 trainloss: 14471.634766\n","Epoch 2, Batch 21 trainloss: 2385.642334\n","Epoch 2, Batch 41 trainloss: 4295.858398\n","Epoch 3, Batch 1 trainloss: 5868.927734\n","Epoch 3, Batch 21 trainloss: 2123.346680\n","Epoch 3, Batch 41 trainloss: 2232.481201\n","Epoch 4, Batch 1 trainloss: 3624.702148\n","Epoch 4, Batch 21 trainloss: 1819.967529\n","Epoch 4, Batch 41 trainloss: 1643.954956\n","Epoch 5, Batch 1 trainloss: 2261.388672\n","Epoch 5, Batch 21 trainloss: 1683.132446\n","Epoch 5, Batch 41 trainloss: 1498.036255\n","Epoch 6, Batch 1 trainloss: 1404.989868\n","Epoch 6, Batch 21 trainloss: 1612.477783\n","Epoch 6, Batch 41 trainloss: 1430.348999\n","Epoch 7, Batch 1 trainloss: 966.078308\n","Epoch 7, Batch 21 trainloss: 1553.796631\n","Epoch 7, Batch 41 trainloss: 1370.442871\n","Epoch 8, Batch 1 trainloss: 777.816406\n","Epoch 8, Batch 21 trainloss: 1497.555420\n","Epoch 8, Batch 41 trainloss: 1343.573486\n","Epoch 9, Batch 1 trainloss: 710.621521\n","Epoch 9, Batch 21 trainloss: 1427.658936\n","Epoch 9, Batch 41 trainloss: 1312.770508\n","Epoch 10, Batch 1 trainloss: 675.197998\n","Epoch 10, Batch 21 trainloss: 1369.774902\n","Epoch 10, Batch 41 trainloss: 1266.115967\n","Epoch 11, Batch 1 trainloss: 672.069031\n","Epoch 11, Batch 21 trainloss: 1314.830078\n","Epoch 11, Batch 41 trainloss: 1237.851807\n","Epoch 12, Batch 1 trainloss: 652.866638\n","Epoch 12, Batch 21 trainloss: 1261.187134\n","Epoch 12, Batch 41 trainloss: 1189.210083\n","Epoch 13, Batch 1 trainloss: 636.686890\n","Epoch 13, Batch 21 trainloss: 1209.959839\n","Epoch 13, Batch 41 trainloss: 1147.305054\n","Epoch 14, Batch 1 trainloss: 617.740845\n","Epoch 14, Batch 21 trainloss: 1162.441040\n","Epoch 14, Batch 41 trainloss: 1103.194702\n","Epoch 15, Batch 1 trainloss: 599.422729\n","Epoch 15, Batch 21 trainloss: 1122.894653\n","Epoch 15, Batch 41 trainloss: 1033.113647\n","Epoch 16, Batch 1 trainloss: 581.200317\n","Epoch 16, Batch 21 trainloss: 1076.773315\n","Epoch 16, Batch 41 trainloss: 1004.910339\n","Epoch 17, Batch 1 trainloss: 562.034302\n","Epoch 17, Batch 21 trainloss: 1038.161011\n","Epoch 17, Batch 41 trainloss: 951.483276\n","Epoch 18, Batch 1 trainloss: 556.410339\n","Epoch 18, Batch 21 trainloss: 1000.293091\n","Epoch 18, Batch 41 trainloss: 906.449402\n","Epoch 19, Batch 1 trainloss: 557.898438\n","Epoch 19, Batch 21 trainloss: 967.354248\n","Epoch 19, Batch 41 trainloss: 857.568420\n","Epoch 20, Batch 1 trainloss: 561.483093\n","Epoch 20, Batch 21 trainloss: 932.158020\n","Epoch 20, Batch 41 trainloss: 814.818848\n","Epoch 21, Batch 1 trainloss: 591.704407\n","Epoch 21, Batch 21 trainloss: 899.975708\n","Epoch 21, Batch 41 trainloss: 752.569824\n","Epoch 22, Batch 1 trainloss: 632.886475\n","Epoch 22, Batch 21 trainloss: 864.976440\n","Epoch 22, Batch 41 trainloss: 720.255920\n","Epoch 23, Batch 1 trainloss: 626.639709\n","Epoch 23, Batch 21 trainloss: 839.136353\n","Epoch 23, Batch 41 trainloss: 692.522400\n","Epoch 24, Batch 1 trainloss: 645.065308\n","Epoch 24, Batch 21 trainloss: 806.533386\n","Epoch 24, Batch 41 trainloss: 655.234192\n","Epoch 25, Batch 1 trainloss: 633.481567\n","Epoch 25, Batch 21 trainloss: 767.729980\n","Epoch 25, Batch 41 trainloss: 610.534607\n","Epoch 26, Batch 1 trainloss: 641.913269\n","Epoch 26, Batch 21 trainloss: 726.773315\n","Epoch 26, Batch 41 trainloss: 606.052124\n","Epoch 27, Batch 1 trainloss: 615.400024\n","Epoch 27, Batch 21 trainloss: 705.709351\n","Epoch 27, Batch 41 trainloss: 606.831604\n","Epoch 28, Batch 1 trainloss: 700.837646\n","Epoch 28, Batch 21 trainloss: 676.956604\n","Epoch 28, Batch 41 trainloss: 593.937378\n","Epoch 29, Batch 1 trainloss: 737.874084\n","Epoch 29, Batch 21 trainloss: 660.567871\n","Epoch 29, Batch 41 trainloss: 590.602783\n","Epoch 30, Batch 1 trainloss: 848.247437\n","Epoch 30, Batch 21 trainloss: 641.580811\n","Epoch 30, Batch 41 trainloss: 577.311279\n","Epoch 31, Batch 1 trainloss: 1108.176147\n","Epoch 31, Batch 21 trainloss: 656.641235\n","Epoch 31, Batch 41 trainloss: 548.925781\n","Epoch 32, Batch 1 trainloss: 1590.678833\n","Epoch 32, Batch 21 trainloss: 714.326965\n","Epoch 32, Batch 41 trainloss: 616.293884\n","Epoch 33, Batch 1 trainloss: 2049.888672\n","Epoch 33, Batch 21 trainloss: 808.233582\n","Epoch 33, Batch 41 trainloss: 593.776306\n","Epoch 34, Batch 1 trainloss: 2970.538818\n","Epoch 34, Batch 21 trainloss: 908.554138\n","Epoch 34, Batch 41 trainloss: 620.395569\n","Epoch 35, Batch 1 trainloss: 3966.077148\n","Epoch 35, Batch 21 trainloss: 856.870300\n","Epoch 35, Batch 41 trainloss: 719.527893\n","Epoch 36, Batch 1 trainloss: 4334.991211\n","Epoch 36, Batch 21 trainloss: 782.569702\n","Epoch 36, Batch 41 trainloss: 866.958801\n","Epoch 37, Batch 1 trainloss: 3793.599365\n","Epoch 37, Batch 21 trainloss: 783.520386\n","Epoch 37, Batch 41 trainloss: 977.862915\n","Epoch 38, Batch 1 trainloss: 3347.004639\n","Epoch 38, Batch 21 trainloss: 1114.263306\n","Epoch 38, Batch 41 trainloss: 497.312378\n","Epoch 39, Batch 1 trainloss: 5341.009277\n","Epoch 39, Batch 21 trainloss: 1614.973877\n","Epoch 39, Batch 41 trainloss: 1945.528442\n","Epoch 40, Batch 1 trainloss: 13235.624023\n","Epoch 40, Batch 21 trainloss: 2979.042236\n","Epoch 40, Batch 41 trainloss: 2891.906494\n","Epoch 41, Batch 1 trainloss: 7951.995605\n","Epoch 41, Batch 21 trainloss: 1643.720825\n","Epoch 41, Batch 41 trainloss: 1086.081299\n","Epoch 42, Batch 1 trainloss: 715.327148\n","Epoch 42, Batch 21 trainloss: 463.725281\n","Epoch 42, Batch 41 trainloss: 1016.445740\n","Epoch 43, Batch 1 trainloss: 1112.892578\n","Epoch 43, Batch 21 trainloss: 950.258728\n","Epoch 43, Batch 41 trainloss: 680.891968\n","Epoch 44, Batch 1 trainloss: 536.720032\n","Epoch 44, Batch 21 trainloss: 657.843445\n","Epoch 44, Batch 41 trainloss: 337.368011\n","Epoch 45, Batch 1 trainloss: 193.555084\n","Epoch 45, Batch 21 trainloss: 396.875427\n","Epoch 45, Batch 41 trainloss: 254.289795\n","Epoch 46, Batch 1 trainloss: 300.361816\n","Epoch 46, Batch 21 trainloss: 322.188721\n","Epoch 46, Batch 41 trainloss: 222.487778\n","Epoch 47, Batch 1 trainloss: 373.669556\n","Epoch 47, Batch 21 trainloss: 299.187805\n","Epoch 47, Batch 41 trainloss: 199.291260\n","Epoch 48, Batch 1 trainloss: 382.530273\n","Epoch 48, Batch 21 trainloss: 289.993317\n","Epoch 48, Batch 41 trainloss: 189.608719\n","Epoch 49, Batch 1 trainloss: 380.842957\n","Epoch 49, Batch 21 trainloss: 281.267975\n","Epoch 49, Batch 41 trainloss: 182.222626\n","Epoch 50, Batch 1 trainloss: 380.852295\n","Epoch 50, Batch 21 trainloss: 278.753906\n","Epoch 50, Batch 41 trainloss: 176.324020\n","Epoch 51, Batch 1 trainloss: 386.874695\n","Epoch 51, Batch 21 trainloss: 275.941071\n","Epoch 51, Batch 41 trainloss: 172.050232\n","Epoch 52, Batch 1 trainloss: 397.973816\n","Epoch 52, Batch 21 trainloss: 274.972534\n","Epoch 52, Batch 41 trainloss: 165.555862\n","Epoch 53, Batch 1 trainloss: 395.486328\n","Epoch 53, Batch 21 trainloss: 272.999390\n","Epoch 53, Batch 41 trainloss: 164.206894\n","Epoch 54, Batch 1 trainloss: 390.073853\n","Epoch 54, Batch 21 trainloss: 269.535034\n","Epoch 54, Batch 41 trainloss: 158.477142\n","Epoch 55, Batch 1 trainloss: 383.438354\n","Epoch 55, Batch 21 trainloss: 266.150787\n","Epoch 55, Batch 41 trainloss: 161.728256\n","Epoch 56, Batch 1 trainloss: 372.735107\n","Epoch 56, Batch 21 trainloss: 270.531433\n","Epoch 56, Batch 41 trainloss: 152.042725\n","Epoch 57, Batch 1 trainloss: 369.572296\n","Epoch 57, Batch 21 trainloss: 270.807159\n","Epoch 57, Batch 41 trainloss: 147.867874\n","Epoch 58, Batch 1 trainloss: 347.513031\n","Epoch 58, Batch 21 trainloss: 275.419159\n","Epoch 58, Batch 41 trainloss: 145.725937\n","Epoch 59, Batch 1 trainloss: 338.929657\n","Epoch 59, Batch 21 trainloss: 275.276337\n","Epoch 59, Batch 41 trainloss: 140.251724\n","Epoch 60, Batch 1 trainloss: 323.692078\n","Epoch 60, Batch 21 trainloss: 280.345917\n","Epoch 60, Batch 41 trainloss: 140.640488\n","Epoch 61, Batch 1 trainloss: 305.036407\n","Epoch 61, Batch 21 trainloss: 283.198975\n","Epoch 61, Batch 41 trainloss: 139.994064\n","Epoch 62, Batch 1 trainloss: 292.552765\n","Epoch 62, Batch 21 trainloss: 285.500885\n","Epoch 62, Batch 41 trainloss: 136.105377\n","Epoch 63, Batch 1 trainloss: 267.284485\n","Epoch 63, Batch 21 trainloss: 286.702240\n","Epoch 63, Batch 41 trainloss: 129.074509\n","Epoch 64, Batch 1 trainloss: 244.741760\n","Epoch 64, Batch 21 trainloss: 287.027405\n","Epoch 64, Batch 41 trainloss: 129.351273\n","Epoch 65, Batch 1 trainloss: 239.631699\n","Epoch 65, Batch 21 trainloss: 294.994598\n","Epoch 65, Batch 41 trainloss: 126.103134\n","Epoch 66, Batch 1 trainloss: 223.937180\n","Epoch 66, Batch 21 trainloss: 294.688538\n","Epoch 66, Batch 41 trainloss: 122.862556\n","Epoch 67, Batch 1 trainloss: 206.730347\n","Epoch 67, Batch 21 trainloss: 294.250397\n","Epoch 67, Batch 41 trainloss: 118.743248\n","Epoch 68, Batch 1 trainloss: 204.871643\n","Epoch 68, Batch 21 trainloss: 303.604767\n","Epoch 68, Batch 41 trainloss: 117.406761\n","Epoch 69, Batch 1 trainloss: 197.007919\n","Epoch 69, Batch 21 trainloss: 307.371643\n","Epoch 69, Batch 41 trainloss: 112.433159\n","Epoch 70, Batch 1 trainloss: 172.872498\n","Epoch 70, Batch 21 trainloss: 309.539551\n","Epoch 70, Batch 41 trainloss: 113.823036\n","Epoch 71, Batch 1 trainloss: 160.779785\n","Epoch 71, Batch 21 trainloss: 319.493469\n","Epoch 71, Batch 41 trainloss: 113.834160\n","Epoch 72, Batch 1 trainloss: 146.250137\n","Epoch 72, Batch 21 trainloss: 322.156403\n","Epoch 72, Batch 41 trainloss: 108.477005\n","Epoch 73, Batch 1 trainloss: 152.364868\n","Epoch 73, Batch 21 trainloss: 323.903320\n","Epoch 73, Batch 41 trainloss: 108.136673\n","Epoch 74, Batch 1 trainloss: 133.449509\n","Epoch 74, Batch 21 trainloss: 340.454407\n","Epoch 74, Batch 41 trainloss: 105.486671\n","Epoch 75, Batch 1 trainloss: 129.497070\n","Epoch 75, Batch 21 trainloss: 351.165314\n","Epoch 75, Batch 41 trainloss: 103.370293\n","Epoch 76, Batch 1 trainloss: 115.229317\n","Epoch 76, Batch 21 trainloss: 353.693756\n","Epoch 76, Batch 41 trainloss: 100.318787\n","Epoch 77, Batch 1 trainloss: 111.909065\n","Epoch 77, Batch 21 trainloss: 375.523163\n","Epoch 77, Batch 41 trainloss: 101.645523\n","Epoch 78, Batch 1 trainloss: 106.930511\n","Epoch 78, Batch 21 trainloss: 393.270172\n","Epoch 78, Batch 41 trainloss: 100.530106\n","Epoch 79, Batch 1 trainloss: 109.022026\n","Epoch 79, Batch 21 trainloss: 409.984802\n","Epoch 79, Batch 41 trainloss: 98.071327\n","Epoch 80, Batch 1 trainloss: 112.341431\n","Epoch 80, Batch 21 trainloss: 418.124481\n","Epoch 80, Batch 41 trainloss: 94.593987\n","Epoch 81, Batch 1 trainloss: 115.000221\n","Epoch 81, Batch 21 trainloss: 422.885132\n","Epoch 81, Batch 41 trainloss: 96.703789\n","Epoch 82, Batch 1 trainloss: 128.259109\n","Epoch 82, Batch 21 trainloss: 435.735046\n","Epoch 82, Batch 41 trainloss: 101.452606\n","Epoch 83, Batch 1 trainloss: 137.655304\n","Epoch 83, Batch 21 trainloss: 438.941254\n","Epoch 83, Batch 41 trainloss: 100.156006\n","Epoch 84, Batch 1 trainloss: 157.698502\n","Epoch 84, Batch 21 trainloss: 453.596008\n","Epoch 84, Batch 41 trainloss: 98.330986\n","Epoch 85, Batch 1 trainloss: 187.823730\n","Epoch 85, Batch 21 trainloss: 460.649384\n","Epoch 85, Batch 41 trainloss: 105.012497\n","Epoch 86, Batch 1 trainloss: 212.501297\n","Epoch 86, Batch 21 trainloss: 470.376556\n","Epoch 86, Batch 41 trainloss: 108.667831\n","Epoch 87, Batch 1 trainloss: 271.253143\n","Epoch 87, Batch 21 trainloss: 460.301605\n","Epoch 87, Batch 41 trainloss: 129.421692\n","Epoch 88, Batch 1 trainloss: 320.269287\n","Epoch 88, Batch 21 trainloss: 463.132935\n","Epoch 88, Batch 41 trainloss: 160.345581\n","Epoch 89, Batch 1 trainloss: 394.894745\n","Epoch 89, Batch 21 trainloss: 451.236053\n","Epoch 89, Batch 41 trainloss: 196.403778\n","Epoch 90, Batch 1 trainloss: 468.275482\n","Epoch 90, Batch 21 trainloss: 418.721863\n","Epoch 90, Batch 41 trainloss: 242.036255\n","Epoch 91, Batch 1 trainloss: 540.715820\n","Epoch 91, Batch 21 trainloss: 376.753662\n","Epoch 91, Batch 41 trainloss: 283.975220\n","Epoch 92, Batch 1 trainloss: 565.688171\n","Epoch 92, Batch 21 trainloss: 307.131836\n","Epoch 92, Batch 41 trainloss: 315.955536\n","Epoch 93, Batch 1 trainloss: 574.987488\n","Epoch 93, Batch 21 trainloss: 236.709564\n","Epoch 93, Batch 41 trainloss: 280.587982\n","Epoch 94, Batch 1 trainloss: 466.404633\n","Epoch 94, Batch 21 trainloss: 163.821259\n","Epoch 94, Batch 41 trainloss: 200.195877\n","Epoch 95, Batch 1 trainloss: 309.817474\n","Epoch 95, Batch 21 trainloss: 114.453842\n","Epoch 95, Batch 41 trainloss: 118.599495\n","Epoch 96, Batch 1 trainloss: 161.636856\n","Epoch 96, Batch 21 trainloss: 98.417725\n","Epoch 96, Batch 41 trainloss: 80.044708\n","Epoch 97, Batch 1 trainloss: 73.265839\n","Epoch 97, Batch 21 trainloss: 98.100555\n","Epoch 97, Batch 41 trainloss: 83.122284\n","Epoch 98, Batch 1 trainloss: 101.368950\n","Epoch 98, Batch 21 trainloss: 100.273727\n","Epoch 98, Batch 41 trainloss: 90.653694\n","Epoch 99, Batch 1 trainloss: 228.861664\n","Epoch 99, Batch 21 trainloss: 102.354607\n","Epoch 99, Batch 41 trainloss: 98.826149\n"]}]},{"cell_type":"code","source":["import pickle\n","with open('losslist.pkl', 'rb') as f:\n","  loss_list3 = pickle.load(f)\n","\n","baseline_loss = loss_list1[2::3]\n","uncertainty_loss = loss_list2[2::3]\n","gradnorm_loss = loss_list3[2::3]\n","training_loss = pd.DataFrame(columns=['baseline', 'uncertainty', 'gradnorm'])\n","training_loss['baseline'] = baseline_loss\n","training_loss['uncertainty'] = uncertainty_loss\n","training_loss['gradnorm'] = gradnorm_loss"],"metadata":{"id":"4JHhtzVcpIIm","executionInfo":{"status":"ok","timestamp":1660021519727,"user_tz":-480,"elapsed":560,"user":{"displayName":"洪睿甫","userId":"05593760976211923971"}}},"execution_count":19,"outputs":[]},{"cell_type":"code","source":["training_loss.loc[90:]"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":363},"id":"kgdnczPdW6gg","executionInfo":{"status":"ok","timestamp":1660021796382,"user_tz":-480,"elapsed":489,"user":{"displayName":"洪睿甫","userId":"05593760976211923971"}},"outputId":"6572d63f-111e-4b80-af74-5b132d4587c8"},"execution_count":27,"outputs":[{"output_type":"execute_result","data":{"text/plain":["       baseline  uncertainty    gradnorm\n","90  1482.713623   242.036255  237.423752\n","91  1454.393555   283.975220  237.541168\n","92  1484.026489   315.955536  237.652802\n","93  1523.307861   280.587982  237.758759\n","94  1628.236572   200.195877  237.859406\n","95  1661.528564   118.599495  237.955109\n","96  1644.606567    80.044708  238.045731\n","97  2109.496582    83.122284  238.132034\n","98  1891.600464    90.653694  238.213745\n","99  2494.294922    98.826149  238.291168"],"text/html":["\n","  <div id=\"df-dd908701-90cc-4815-9fe3-ddcadcac7cbf\">\n","    <div class=\"colab-df-container\">\n","      <div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>baseline</th>\n","      <th>uncertainty</th>\n","      <th>gradnorm</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>90</th>\n","      <td>1482.713623</td>\n","      <td>242.036255</td>\n","      <td>237.423752</td>\n","    </tr>\n","    <tr>\n","      <th>91</th>\n","      <td>1454.393555</td>\n","      <td>283.975220</td>\n","      <td>237.541168</td>\n","    </tr>\n","    <tr>\n","      <th>92</th>\n","      <td>1484.026489</td>\n","      <td>315.955536</td>\n","      <td>237.652802</td>\n","    </tr>\n","    <tr>\n","      <th>93</th>\n","      <td>1523.307861</td>\n","      <td>280.587982</td>\n","      <td>237.758759</td>\n","    </tr>\n","    <tr>\n","      <th>94</th>\n","      <td>1628.236572</td>\n","      <td>200.195877</td>\n","      <td>237.859406</td>\n","    </tr>\n","    <tr>\n","      <th>95</th>\n","      <td>1661.528564</td>\n","      <td>118.599495</td>\n","      <td>237.955109</td>\n","    </tr>\n","    <tr>\n","      <th>96</th>\n","      <td>1644.606567</td>\n","      <td>80.044708</td>\n","      <td>238.045731</td>\n","    </tr>\n","    <tr>\n","      <th>97</th>\n","      <td>2109.496582</td>\n","      <td>83.122284</td>\n","      <td>238.132034</td>\n","    </tr>\n","    <tr>\n","      <th>98</th>\n","      <td>1891.600464</td>\n","      <td>90.653694</td>\n","      <td>238.213745</td>\n","    </tr>\n","    <tr>\n","      <th>99</th>\n","      <td>2494.294922</td>\n","      <td>98.826149</td>\n","      <td>238.291168</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>\n","      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-dd908701-90cc-4815-9fe3-ddcadcac7cbf')\"\n","              title=\"Convert this dataframe to an interactive table.\"\n","              style=\"display:none;\">\n","        \n","  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n","       width=\"24px\">\n","    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n","    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n","  </svg>\n","      </button>\n","      \n","  <style>\n","    .colab-df-container {\n","      display:flex;\n","      flex-wrap:wrap;\n","      gap: 12px;\n","    }\n","\n","    .colab-df-convert {\n","      background-color: #E8F0FE;\n","      border: none;\n","      border-radius: 50%;\n","      cursor: pointer;\n","      display: none;\n","      fill: #1967D2;\n","      height: 32px;\n","      padding: 0 0 0 0;\n","      width: 32px;\n","    }\n","\n","    .colab-df-convert:hover {\n","      background-color: #E2EBFA;\n","      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n","      fill: #174EA6;\n","    }\n","\n","    [theme=dark] .colab-df-convert {\n","      background-color: #3B4455;\n","      fill: #D2E3FC;\n","    }\n","\n","    [theme=dark] .colab-df-convert:hover {\n","      background-color: #434B5C;\n","      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n","      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n","      fill: #FFFFFF;\n","    }\n","  </style>\n","\n","      <script>\n","        const buttonEl =\n","          document.querySelector('#df-dd908701-90cc-4815-9fe3-ddcadcac7cbf button.colab-df-convert');\n","        buttonEl.style.display =\n","          google.colab.kernel.accessAllowed ? 'block' : 'none';\n","\n","        async function convertToInteractive(key) {\n","          const element = document.querySelector('#df-dd908701-90cc-4815-9fe3-ddcadcac7cbf');\n","          const dataTable =\n","            await google.colab.kernel.invokeFunction('convertToInteractive',\n","                                                     [key], {});\n","          if (!dataTable) return;\n","\n","          const docLinkHtml = 'Like what you see? Visit the ' +\n","            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n","            + ' to learn more about interactive tables.';\n","          element.innerHTML = '';\n","          dataTable['output_type'] = 'display_data';\n","          await google.colab.output.renderOutput(dataTable, element);\n","          const docLink = document.createElement('div');\n","          docLink.innerHTML = docLinkHtml;\n","          element.appendChild(docLink);\n","        }\n","      </script>\n","    </div>\n","  </div>\n","  "]},"metadata":{},"execution_count":27}]},{"cell_type":"code","source":["import seaborn as sns\n","import matplotlib.pyplot as plt\n","plt.figure(figsize=(7, 5))\n","sns.lineplot(data=training_loss)\n","plt.xlabel(\"Epoch\")\n","plt.ylabel(\"Loss value\")\n","plt.show()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":334},"id":"uIysk5gGogE0","executionInfo":{"status":"ok","timestamp":1660021545058,"user_tz":-480,"elapsed":1360,"user":{"displayName":"洪睿甫","userId":"05593760976211923971"}},"outputId":"c9124e4d-9aea-4298-f08c-27b3d4bc82a6"},"execution_count":22,"outputs":[{"output_type":"display_data","data":{"text/plain":["<Figure size 504x360 with 1 Axes>"],"image/png":"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\n"},"metadata":{"needs_background":"light"}}]},{"cell_type":"code","source":["print(torch.exp(log_var_dict['透析方式'])**0.5, torch.exp(log_var_dict['抗凝劑'])**0.5)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"hLOFvUxwPCJM","executionInfo":{"status":"ok","timestamp":1658130453541,"user_tz":-480,"elapsed":443,"user":{"displayName":"洪睿甫","userId":"05593760976211923971"}},"outputId":"86f95416-5ce5-46bd-f497-5c3a817dd9cc"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["tensor([0.9751], grad_fn=<PowBackward0>) tensor([1.0539], grad_fn=<PowBackward0>)\n"]}]},{"cell_type":"code","source":["# testtttttttttttttttttt\n","\n","class Net2(nn.Module):\n","  def __init__(self, unit_dict):\n","    super(Net2, self).__init__()\n","    self.layer1 = nn.Linear(25, 64) \n","    self.relu = nn.ReLU()\n","    self.layer2 = nn.Linear(64, 32)\n","    \n","    self.branch5 = nn.Linear(32, 1)\n","    self.branch6 = nn.Linear(32, 1)\n","    \n","  def forward(self, x):\n","    x = self.layer1(x)\n","    x = self.relu(x)\n","    x = self.layer2(x)\n","    \n","    label5 = self.branch5(x)\n","    label6 = self.branch6(x)\n","    \n","\n","    return {'血液流速' : label5, '透析液流速' : label6}\n","model2 = Net2(unit_dict)"],"metadata":{"id":"TPJSHW1hTdEm"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def criteriontest(y_pred, y_true, log_vars, loss_func):\n","  loss = 0\n","  precision = torch.exp(-log_vars)\n","  loss_value = loss_func(y_pred,y_true.squeeze().type(torch.LongTensor))\n","  loss = torch.sum(precision * loss_value + log_vars, -1)\n","  return loss"],"metadata":{"id":"UAJArZflVqA1"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import torch.nn.functional as F\n","for epoch in range(1, 10):\n","    train_loss = 0.0\n","    # train the model #\n","    model2.train()\n","    for index, dataset in enumerate(dataloader):\n","        loss = 0\n","        patient_state = dataset['patient_state']\n","        # zero the parameter gradients\n","        optimizer.zero_grad()\n","        output=model2(patient_state)\n","        \n","        for i in list(dataset.keys())[5:7]:\n","          label = dataset[i]\n","          label_hat = output[i]\n","          log_var = log_var_dict[i]\n","          if i == '血液流速' or i == '透析液流速':\n","            #label_hat = label_hat.squeeze()\n","            \n","            #print(label,label_hat)\n","            loss += criteriontest(label_hat,label,log_var,loss_func['continuous1'])\n","            print(loss)\n","          else:\n","            loss += criteriontest(label_hat,label,log_var,loss_func['categorical'])\n","\n","        loss = torch.mean(loss)\n","        \n","        # back prop\n","        loss.backward() \n","        # grad\n","        optimizer.step()\n","        train_loss = train_loss + ((1 / (index + 1)) * (loss.data - train_loss))\n","        if index % 20 == 0:\n","            print('Epoch %d, Batch %d trainloss: %.6f loss: %.6f' %\n","              (epoch, index + 1, train_loss, loss))\n","        \n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"0BV493c4TpcQ","executionInfo":{"status":"ok","timestamp":1658131983012,"user_tz":-480,"elapsed":8248,"user":{"displayName":"洪睿甫","userId":"05593760976211923971"}},"outputId":"42951f55-ea6d-4d2e-a76d-4b3a0abe3d5d"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/torch/nn/modules/loss.py:96: UserWarning: Using a target size (torch.Size([4])) that is different to the input size (torch.Size([4, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n","  return F.l1_loss(input, target, reduction=self.reduction)\n"]},{"output_type":"stream","name":"stdout","text":["tensor(112.0746, grad_fn=<AddBackward0>)\n","tensor(335.0180, grad_fn=<AddBackward0>)\n","Epoch 1, Batch 1 trainloss: 335.017975 loss: 335.017975\n","tensor(120.5393, grad_fn=<AddBackward0>)\n","tensor(345.6575, grad_fn=<AddBackward0>)\n","tensor(103.7015, grad_fn=<AddBackward0>)\n","tensor(333.3466, grad_fn=<AddBackward0>)\n","tensor(131.2315, grad_fn=<AddBackward0>)\n","tensor(356.4425, grad_fn=<AddBackward0>)\n","tensor(119.1302, grad_fn=<AddBackward0>)\n","tensor(341.0171, grad_fn=<AddBackward0>)\n","tensor(120.8108, grad_fn=<AddBackward0>)\n","tensor(345.5430, grad_fn=<AddBackward0>)\n","tensor(129.9182, grad_fn=<AddBackward0>)\n","tensor(354.4366, grad_fn=<AddBackward0>)\n","tensor(120.9556, grad_fn=<AddBackward0>)\n","tensor(345.3024, grad_fn=<AddBackward0>)\n","tensor(122.0823, grad_fn=<AddBackward0>)\n","tensor(348.1743, grad_fn=<AddBackward0>)\n","tensor(138.6451, grad_fn=<AddBackward0>)\n","tensor(361.4201, grad_fn=<AddBackward0>)\n","tensor(139.1949, grad_fn=<AddBackward0>)\n","tensor(363.1295, grad_fn=<AddBackward0>)\n","tensor(125.1925, grad_fn=<AddBackward0>)\n","tensor(344.8003, grad_fn=<AddBackward0>)\n","tensor(118.8031, grad_fn=<AddBackward0>)\n","tensor(342.2625, grad_fn=<AddBackward0>)\n","tensor(140.4990, grad_fn=<AddBackward0>)\n","tensor(363.7265, grad_fn=<AddBackward0>)\n","tensor(135.8821, grad_fn=<AddBackward0>)\n","tensor(357.2374, grad_fn=<AddBackward0>)\n","tensor(112.9135, grad_fn=<AddBackward0>)\n","tensor(332.5280, grad_fn=<AddBackward0>)\n","tensor(121.0258, grad_fn=<AddBackward0>)\n","tensor(344.0858, grad_fn=<AddBackward0>)\n","tensor(132.1403, grad_fn=<AddBackward0>)\n","tensor(355.8653, grad_fn=<AddBackward0>)\n","tensor(130.9283, grad_fn=<AddBackward0>)\n","tensor(374.2003, grad_fn=<AddBackward0>)\n","tensor(134.3200, grad_fn=<AddBackward0>)\n","tensor(357.8030, grad_fn=<AddBackward0>)\n","tensor(135.4462, grad_fn=<AddBackward0>)\n","tensor(358.1179, grad_fn=<AddBackward0>)\n","Epoch 1, Batch 21 trainloss: 350.481628 loss: 358.117920\n","tensor(128.7710, grad_fn=<AddBackward0>)\n","tensor(352.3729, grad_fn=<AddBackward0>)\n","tensor(127.5467, grad_fn=<AddBackward0>)\n","tensor(347.4469, grad_fn=<AddBackward0>)\n","tensor(125.5855, grad_fn=<AddBackward0>)\n","tensor(347.3809, grad_fn=<AddBackward0>)\n","tensor(132.4314, grad_fn=<AddBackward0>)\n","tensor(384.5569, grad_fn=<AddBackward0>)\n","tensor(129.3877, grad_fn=<AddBackward0>)\n","tensor(343.7376, grad_fn=<AddBackward0>)\n","tensor(120.1505, grad_fn=<AddBackward0>)\n","tensor(341.1177, grad_fn=<AddBackward0>)\n","tensor(118.3062, grad_fn=<AddBackward0>)\n","tensor(333.8771, grad_fn=<AddBackward0>)\n","tensor(118.1176, grad_fn=<AddBackward0>)\n","tensor(338.6988, grad_fn=<AddBackward0>)\n","tensor(126.2778, grad_fn=<AddBackward0>)\n","tensor(346.6294, grad_fn=<AddBackward0>)\n","tensor(136.2566, grad_fn=<AddBackward0>)\n","tensor(356.4386, grad_fn=<AddBackward0>)\n","tensor(126.3813, grad_fn=<AddBackward0>)\n","tensor(346.3816, grad_fn=<AddBackward0>)\n","tensor(110.2700, grad_fn=<AddBackward0>)\n","tensor(330.1476, grad_fn=<AddBackward0>)\n","tensor(121.6640, grad_fn=<AddBackward0>)\n","tensor(374.1507, grad_fn=<AddBackward0>)\n","tensor(122.0896, grad_fn=<AddBackward0>)\n","tensor(355.2429, grad_fn=<AddBackward0>)\n","tensor(106.2836, grad_fn=<AddBackward0>)\n","tensor(325.0530, grad_fn=<AddBackward0>)\n","tensor(111.9624, grad_fn=<AddBackward0>)\n","tensor(329.9472, grad_fn=<AddBackward0>)\n","tensor(124.4698, grad_fn=<AddBackward0>)\n","tensor(343.4069, grad_fn=<AddBackward0>)\n","tensor(127.7155, grad_fn=<AddBackward0>)\n","tensor(340.3588, grad_fn=<AddBackward0>)\n","tensor(135.4767, grad_fn=<AddBackward0>)\n","tensor(352.4296, grad_fn=<AddBackward0>)\n","tensor(119.9827, grad_fn=<AddBackward0>)\n","tensor(339.2791, grad_fn=<AddBackward0>)\n","Epoch 1, Batch 41 trainloss: 348.506531 loss: 339.279083\n","tensor(108.1568, grad_fn=<AddBackward0>)\n","tensor(324.9279, grad_fn=<AddBackward0>)\n","tensor(105.7238, grad_fn=<AddBackward0>)\n","tensor(323.8212, grad_fn=<AddBackward0>)\n","tensor(134.6186, grad_fn=<AddBackward0>)\n","tensor(353.6041, grad_fn=<AddBackward0>)\n","tensor(127.1930, grad_fn=<AddBackward0>)\n","tensor(343.4760, grad_fn=<AddBackward0>)\n","tensor(124.7574, grad_fn=<AddBackward0>)\n","tensor(342.2957, grad_fn=<AddBackward0>)\n","tensor(110.7232, grad_fn=<AddBackward0>)\n","tensor(323.9684, grad_fn=<AddBackward0>)\n","tensor(113.0072, grad_fn=<AddBackward0>)\n","tensor(331.1629, grad_fn=<AddBackward0>)\n","tensor(110.5601, grad_fn=<AddBackward0>)\n","tensor(327.6175, grad_fn=<AddBackward0>)\n","tensor(107.5675, grad_fn=<AddBackward0>)\n","tensor(322.1995, grad_fn=<AddBackward0>)\n","tensor(110.6394, grad_fn=<AddBackward0>)\n","tensor(348.0565, grad_fn=<AddBackward0>)\n","tensor(118.1789, grad_fn=<AddBackward0>)\n","tensor(336.3638, grad_fn=<AddBackward0>)\n","tensor(110.4363, grad_fn=<AddBackward0>)\n","tensor(322.3878, grad_fn=<AddBackward0>)\n","tensor(117.4559, grad_fn=<AddBackward0>)\n","tensor(333.7307, grad_fn=<AddBackward0>)\n","tensor(101.4237, grad_fn=<AddBackward0>)\n","tensor(314.5239, grad_fn=<AddBackward0>)\n","tensor(110.0833, grad_fn=<AddBackward0>)\n","tensor(319.1201, grad_fn=<AddBackward0>)\n","tensor(138.6966, grad_fn=<AddBackward0>)\n","tensor(351.7548, grad_fn=<AddBackward0>)\n","tensor(104.8249, grad_fn=<AddBackward0>)\n","tensor(319.7585, grad_fn=<AddBackward0>)\n","tensor(109.1030, grad_fn=<AddBackward0>)\n","tensor(325.1539, grad_fn=<AddBackward0>)\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/torch/nn/modules/loss.py:96: UserWarning: Using a target size (torch.Size([3])) that is different to the input size (torch.Size([3, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n","  return F.l1_loss(input, target, reduction=self.reduction)\n"]},{"output_type":"stream","name":"stdout","text":["tensor(106.9309, grad_fn=<AddBackward0>)\n","tensor(319.5689, grad_fn=<AddBackward0>)\n","Epoch 2, Batch 1 trainloss: 319.568909 loss: 319.568909\n","tensor(115.0036, grad_fn=<AddBackward0>)\n","tensor(329.7206, grad_fn=<AddBackward0>)\n","tensor(98.9535, grad_fn=<AddBackward0>)\n","tensor(317.9923, grad_fn=<AddBackward0>)\n","tensor(125.2035, grad_fn=<AddBackward0>)\n","tensor(340.0201, grad_fn=<AddBackward0>)\n","tensor(113.6688, grad_fn=<AddBackward0>)\n","tensor(325.3215, grad_fn=<AddBackward0>)\n","tensor(115.2740, grad_fn=<AddBackward0>)\n","tensor(329.6452, grad_fn=<AddBackward0>)\n","tensor(123.9608, grad_fn=<AddBackward0>)\n","tensor(338.1336, grad_fn=<AddBackward0>)\n","tensor(115.4179, grad_fn=<AddBackward0>)\n","tensor(329.4327, grad_fn=<AddBackward0>)\n","tensor(116.4953, grad_fn=<AddBackward0>)\n","tensor(332.1797, grad_fn=<AddBackward0>)\n","tensor(132.2918, grad_fn=<AddBackward0>)\n","tensor(344.8186, grad_fn=<AddBackward0>)\n","tensor(132.8195, grad_fn=<AddBackward0>)\n","tensor(346.4576, grad_fn=<AddBackward0>)\n","tensor(119.4702, grad_fn=<AddBackward0>)\n","tensor(328.9874, grad_fn=<AddBackward0>)\n","tensor(113.3801, grad_fn=<AddBackward0>)\n","tensor(326.5759, grad_fn=<AddBackward0>)\n","tensor(134.0735, grad_fn=<AddBackward0>)\n","tensor(347.0535, grad_fn=<AddBackward0>)\n","tensor(129.6740, grad_fn=<AddBackward0>)\n","tensor(340.8738, grad_fn=<AddBackward0>)\n","tensor(107.7720, grad_fn=<AddBackward0>)\n","tensor(317.3169, grad_fn=<AddBackward0>)\n","tensor(115.5118, grad_fn=<AddBackward0>)\n","tensor(328.3482, grad_fn=<AddBackward0>)\n","tensor(126.1153, grad_fn=<AddBackward0>)\n","tensor(339.5914, grad_fn=<AddBackward0>)\n","tensor(124.9626, grad_fn=<AddBackward0>)\n","tensor(357.0887, grad_fn=<AddBackward0>)\n","tensor(128.2009, grad_fn=<AddBackward0>)\n","tensor(341.4569, grad_fn=<AddBackward0>)\n","tensor(129.2785, grad_fn=<AddBackward0>)\n","tensor(341.7660, grad_fn=<AddBackward0>)\n","Epoch 2, Batch 21 trainloss: 334.397552 loss: 341.765991\n","tensor(122.9147, grad_fn=<AddBackward0>)\n","tensor(336.2949, grad_fn=<AddBackward0>)\n","tensor(121.7501, grad_fn=<AddBackward0>)\n","tensor(331.6044, grad_fn=<AddBackward0>)\n","tensor(119.8826, grad_fn=<AddBackward0>)\n","tensor(331.5500, grad_fn=<AddBackward0>)\n","tensor(126.4162, grad_fn=<AddBackward0>)\n","tensor(367.0233, grad_fn=<AddBackward0>)\n","tensor(123.5159, grad_fn=<AddBackward0>)\n","tensor(328.0911, grad_fn=<AddBackward0>)\n","tensor(114.7070, grad_fn=<AddBackward0>)\n","tensor(325.6003, grad_fn=<AddBackward0>)\n","tensor(112.9505, grad_fn=<AddBackward0>)\n","tensor(318.7008, grad_fn=<AddBackward0>)\n","tensor(112.7735, grad_fn=<AddBackward0>)\n","tensor(323.3091, grad_fn=<AddBackward0>)\n","tensor(120.5618, grad_fn=<AddBackward0>)\n","tensor(330.8835, grad_fn=<AddBackward0>)\n","tensor(130.0855, grad_fn=<AddBackward0>)\n","tensor(340.2505, grad_fn=<AddBackward0>)\n","tensor(120.6667, grad_fn=<AddBackward0>)\n","tensor(330.6636, grad_fn=<AddBackward0>)\n","tensor(105.2975, grad_fn=<AddBackward0>)\n","tensor(315.1825, grad_fn=<AddBackward0>)\n","tensor(116.1718, grad_fn=<AddBackward0>)\n","tensor(357.1772, grad_fn=<AddBackward0>)\n","tensor(116.5808, grad_fn=<AddBackward0>)\n","tensor(339.1441, grad_fn=<AddBackward0>)\n","tensor(101.5016, grad_fn=<AddBackward0>)\n","tensor(310.3447, grad_fn=<AddBackward0>)\n","tensor(106.9230, grad_fn=<AddBackward0>)\n","tensor(315.0225, grad_fn=<AddBackward0>)\n","tensor(118.8608, grad_fn=<AddBackward0>)\n","tensor(327.8742, grad_fn=<AddBackward0>)\n","tensor(121.9611, grad_fn=<AddBackward0>)\n","tensor(324.9734, grad_fn=<AddBackward0>)\n","tensor(129.3707, grad_fn=<AddBackward0>)\n","tensor(336.5007, grad_fn=<AddBackward0>)\n","tensor(114.5875, grad_fn=<AddBackward0>)\n","tensor(323.9592, grad_fn=<AddBackward0>)\n","Epoch 2, Batch 41 trainloss: 332.597565 loss: 323.959167\n","tensor(103.3044, grad_fn=<AddBackward0>)\n","tensor(310.2709, grad_fn=<AddBackward0>)\n","tensor(100.9850, grad_fn=<AddBackward0>)\n","tensor(309.2223, grad_fn=<AddBackward0>)\n","tensor(128.5644, grad_fn=<AddBackward0>)\n","tensor(337.6544, grad_fn=<AddBackward0>)\n","tensor(121.4804, grad_fn=<AddBackward0>)\n","tensor(327.9960, grad_fn=<AddBackward0>)\n","tensor(119.1587, grad_fn=<AddBackward0>)\n","tensor(326.8775, grad_fn=<AddBackward0>)\n","tensor(105.7665, grad_fn=<AddBackward0>)\n","tensor(309.3925, grad_fn=<AddBackward0>)\n","tensor(107.9492, grad_fn=<AddBackward0>)\n","tensor(316.2671, grad_fn=<AddBackward0>)\n","tensor(105.6160, grad_fn=<AddBackward0>)\n","tensor(312.8904, grad_fn=<AddBackward0>)\n","tensor(102.7619, grad_fn=<AddBackward0>)\n","tensor(307.7261, grad_fn=<AddBackward0>)\n","tensor(105.6966, grad_fn=<AddBackward0>)\n","tensor(332.4160, grad_fn=<AddBackward0>)\n","tensor(112.8962, grad_fn=<AddBackward0>)\n","tensor(321.2617, grad_fn=<AddBackward0>)\n","tensor(105.5077, grad_fn=<AddBackward0>)\n","tensor(307.9276, grad_fn=<AddBackward0>)\n","tensor(112.2112, grad_fn=<AddBackward0>)\n","tensor(318.7631, grad_fn=<AddBackward0>)\n","tensor(96.9088, grad_fn=<AddBackward0>)\n","tensor(300.4348, grad_fn=<AddBackward0>)\n","tensor(105.1780, grad_fn=<AddBackward0>)\n","tensor(304.8295, grad_fn=<AddBackward0>)\n","tensor(132.4971, grad_fn=<AddBackward0>)\n","tensor(335.9927, grad_fn=<AddBackward0>)\n","tensor(100.1626, grad_fn=<AddBackward0>)\n","tensor(305.4534, grad_fn=<AddBackward0>)\n","tensor(104.2493, grad_fn=<AddBackward0>)\n","tensor(310.6117, grad_fn=<AddBackward0>)\n","tensor(102.1779, grad_fn=<AddBackward0>)\n","tensor(305.2864, grad_fn=<AddBackward0>)\n","Epoch 3, Batch 1 trainloss: 305.286377 loss: 305.286377\n","tensor(109.8878, grad_fn=<AddBackward0>)\n","tensor(314.9861, grad_fn=<AddBackward0>)\n","tensor(94.5656, grad_fn=<AddBackward0>)\n","tensor(303.7952, grad_fn=<AddBackward0>)\n","tensor(119.6317, grad_fn=<AddBackward0>)\n","tensor(324.8346, grad_fn=<AddBackward0>)\n","tensor(108.6206, grad_fn=<AddBackward0>)\n","tensor(310.8070, grad_fn=<AddBackward0>)\n","tensor(110.1559, grad_fn=<AddBackward0>)\n","tensor(314.9428, grad_fn=<AddBackward0>)\n","tensor(118.4532, grad_fn=<AddBackward0>)\n","tensor(323.0554, grad_fn=<AddBackward0>)\n","tensor(110.2982, grad_fn=<AddBackward0>)\n","tensor(314.7542, grad_fn=<AddBackward0>)\n","tensor(111.3295, grad_fn=<AddBackward0>)\n","tensor(317.3845, grad_fn=<AddBackward0>)\n","tensor(126.4168, grad_fn=<AddBackward0>)\n","tensor(329.4612, grad_fn=<AddBackward0>)\n","tensor(126.9237, grad_fn=<AddBackward0>)\n","tensor(331.0339, grad_fn=<AddBackward0>)\n","tensor(114.1781, grad_fn=<AddBackward0>)\n","tensor(314.3574, grad_fn=<AddBackward0>)\n","tensor(108.3646, grad_fn=<AddBackward0>)\n","tensor(312.0616, grad_fn=<AddBackward0>)\n","tensor(128.1301, grad_fn=<AddBackward0>)\n","tensor(331.6256, grad_fn=<AddBackward0>)\n","tensor(123.9313, grad_fn=<AddBackward0>)\n","tensor(325.7311, grad_fn=<AddBackward0>)\n","tensor(103.0160, grad_fn=<AddBackward0>)\n","tensor(303.2398, grad_fn=<AddBackward0>)\n","tensor(110.4107, grad_fn=<AddBackward0>)\n","tensor(313.7828, grad_fn=<AddBackward0>)\n","tensor(120.5410, grad_fn=<AddBackward0>)\n","tensor(324.5285, grad_fn=<AddBackward0>)\n","tensor(119.4427, grad_fn=<AddBackward0>)\n","tensor(341.2489, grad_fn=<AddBackward0>)\n","tensor(122.5386, grad_fn=<AddBackward0>)\n","tensor(326.3252, grad_fn=<AddBackward0>)\n","tensor(123.5708, grad_fn=<AddBackward0>)\n","tensor(326.6278, grad_fn=<AddBackward0>)\n","Epoch 3, Batch 21 trainloss: 319.517609 loss: 326.627808\n","tensor(117.4949, grad_fn=<AddBackward0>)\n","tensor(321.4092, grad_fn=<AddBackward0>)\n","tensor(116.3852, grad_fn=<AddBackward0>)\n","tensor(316.9360, grad_fn=<AddBackward0>)\n","tensor(114.6039, grad_fn=<AddBackward0>)\n","tensor(316.8912, grad_fn=<AddBackward0>)\n","tensor(120.8481, grad_fn=<AddBackward0>)\n","tensor(350.7861, grad_fn=<AddBackward0>)\n","tensor(118.0802, grad_fn=<AddBackward0>)\n","tensor(313.6011, grad_fn=<AddBackward0>)\n","tensor(109.6676, grad_fn=<AddBackward0>)\n","tensor(311.2289, grad_fn=<AddBackward0>)\n","tensor(107.9921, grad_fn=<AddBackward0>)\n","tensor(304.6444, grad_fn=<AddBackward0>)\n","tensor(107.8255, grad_fn=<AddBackward0>)\n","tensor(309.0540, grad_fn=<AddBackward0>)\n","tensor(115.2688, grad_fn=<AddBackward0>)\n","tensor(316.2974, grad_fn=<AddBackward0>)\n","tensor(124.3705, grad_fn=<AddBackward0>)\n","tensor(325.2539, grad_fn=<AddBackward0>)\n","tensor(115.3743, grad_fn=<AddBackward0>)\n","tensor(316.1015, grad_fn=<AddBackward0>)\n","tensor(100.6924, grad_fn=<AddBackward0>)\n","tensor(301.3170, grad_fn=<AddBackward0>)\n","tensor(111.0849, grad_fn=<AddBackward0>)\n","tensor(341.4494, grad_fn=<AddBackward0>)\n","tensor(111.4782, grad_fn=<AddBackward0>)\n","tensor(324.2260, grad_fn=<AddBackward0>)\n","tensor(97.0721, grad_fn=<AddBackward0>)\n","tensor(296.7145, grad_fn=<AddBackward0>)\n","tensor(102.2547, grad_fn=<AddBackward0>)\n","tensor(301.1909, grad_fn=<AddBackward0>)\n","tensor(113.6643, grad_fn=<AddBackward0>)\n","tensor(313.4782, grad_fn=<AddBackward0>)\n","tensor(116.6293, grad_fn=<AddBackward0>)\n","tensor(310.7130, grad_fn=<AddBackward0>)\n","tensor(123.7126, grad_fn=<AddBackward0>)\n","tensor(321.7354, grad_fn=<AddBackward0>)\n","tensor(109.5881, grad_fn=<AddBackward0>)\n","tensor(309.7574, grad_fn=<AddBackward0>)\n","Epoch 3, Batch 41 trainloss: 317.869598 loss: 309.757446\n","tensor(98.8078, grad_fn=<AddBackward0>)\n","tensor(296.6830, grad_fn=<AddBackward0>)\n","tensor(96.5934, grad_fn=<AddBackward0>)\n","tensor(295.6873, grad_fn=<AddBackward0>)\n","tensor(122.9529, grad_fn=<AddBackward0>)\n","tensor(322.8661, grad_fn=<AddBackward0>)\n","tensor(116.1853, grad_fn=<AddBackward0>)\n","tensor(313.6424, grad_fn=<AddBackward0>)\n","tensor(113.9689, grad_fn=<AddBackward0>)\n","tensor(312.5801, grad_fn=<AddBackward0>)\n","tensor(101.1717, grad_fn=<AddBackward0>)\n","tensor(295.8754, grad_fn=<AddBackward0>)\n","tensor(103.2601, grad_fn=<AddBackward0>)\n","tensor(302.4525, grad_fn=<AddBackward0>)\n","tensor(101.0322, grad_fn=<AddBackward0>)\n","tensor(299.2315, grad_fn=<AddBackward0>)\n","tensor(98.3064, grad_fn=<AddBackward0>)\n","tensor(294.3017, grad_fn=<AddBackward0>)\n","tensor(101.1136, grad_fn=<AddBackward0>)\n","tensor(317.9077, grad_fn=<AddBackward0>)\n","tensor(107.9975, grad_fn=<AddBackward0>)\n","tensor(307.2523, grad_fn=<AddBackward0>)\n","tensor(100.9372, grad_fn=<AddBackward0>)\n","tensor(294.5129, grad_fn=<AddBackward0>)\n","tensor(107.3472, grad_fn=<AddBackward0>)\n","tensor(304.8768, grad_fn=<AddBackward0>)\n","tensor(92.7216, grad_fn=<AddBackward0>)\n","tensor(287.3629, grad_fn=<AddBackward0>)\n","tensor(100.6284, grad_fn=<AddBackward0>)\n","tensor(291.5699, grad_fn=<AddBackward0>)\n","tensor(126.7463, grad_fn=<AddBackward0>)\n","tensor(321.3666, grad_fn=<AddBackward0>)\n","tensor(95.8379, grad_fn=<AddBackward0>)\n","tensor(292.1784, grad_fn=<AddBackward0>)\n","tensor(99.7467, grad_fn=<AddBackward0>)\n","tensor(297.1158, grad_fn=<AddBackward0>)\n","tensor(97.7684, grad_fn=<AddBackward0>)\n","tensor(292.0308, grad_fn=<AddBackward0>)\n","Epoch 4, Batch 1 trainloss: 292.030792 loss: 292.030792\n","tensor(105.1414, grad_fn=<AddBackward0>)\n","tensor(301.3101, grad_fn=<AddBackward0>)\n","tensor(90.4947, grad_fn=<AddBackward0>)\n","tensor(290.6173, grad_fn=<AddBackward0>)\n","tensor(114.4615, grad_fn=<AddBackward0>)\n","tensor(310.7383, grad_fn=<AddBackward0>)\n","tensor(103.9362, grad_fn=<AddBackward0>)\n","tensor(297.3330, grad_fn=<AddBackward0>)\n","tensor(105.4061, grad_fn=<AddBackward0>)\n","tensor(301.2934, grad_fn=<AddBackward0>)\n","tensor(113.3416, grad_fn=<AddBackward0>)\n","tensor(309.0562, grad_fn=<AddBackward0>)\n","tensor(105.5465, grad_fn=<AddBackward0>)\n","tensor(301.1253, grad_fn=<AddBackward0>)\n","tensor(106.5347, grad_fn=<AddBackward0>)\n","tensor(303.6465, grad_fn=<AddBackward0>)\n","tensor(120.9633, grad_fn=<AddBackward0>)\n","tensor(315.2003, grad_fn=<AddBackward0>)\n","tensor(121.4504, grad_fn=<AddBackward0>)\n","tensor(316.7106, grad_fn=<AddBackward0>)\n","tensor(109.2652, grad_fn=<AddBackward0>)\n","tensor(300.7704, grad_fn=<AddBackward0>)\n","tensor(103.7084, grad_fn=<AddBackward0>)\n","tensor(298.5814, grad_fn=<AddBackward0>)\n","tensor(122.6116, grad_fn=<AddBackward0>)\n","tensor(317.2960, grad_fn=<AddBackward0>)\n","tensor(118.5989, grad_fn=<AddBackward0>)\n","tensor(311.6656, grad_fn=<AddBackward0>)\n","tensor(98.5999, grad_fn=<AddBackward0>)\n","tensor(290.1632, grad_fn=<AddBackward0>)\n","tensor(105.6737, grad_fn=<AddBackward0>)\n","tensor(300.2518, grad_fn=<AddBackward0>)\n","tensor(115.3640, grad_fn=<AddBackward0>)\n","tensor(310.5346, grad_fn=<AddBackward0>)\n","tensor(114.3161, grad_fn=<AddBackward0>)\n","tensor(326.5321, grad_fn=<AddBackward0>)\n","tensor(117.2793, grad_fn=<AddBackward0>)\n","tensor(312.2656, grad_fn=<AddBackward0>)\n","tensor(118.2690, grad_fn=<AddBackward0>)\n","tensor(312.5615, grad_fn=<AddBackward0>)\n","Epoch 4, Batch 21 trainloss: 305.699219 loss: 312.561462\n","tensor(112.4603, grad_fn=<AddBackward0>)\n","tensor(307.5766, grad_fn=<AddBackward0>)\n","tensor(111.4013, grad_fn=<AddBackward0>)\n","tensor(303.3045, grad_fn=<AddBackward0>)\n","tensor(109.6999, grad_fn=<AddBackward0>)\n","tensor(303.2679, grad_fn=<AddBackward0>)\n","tensor(115.6747, grad_fn=<AddBackward0>)\n","tensor(335.6945, grad_fn=<AddBackward0>)\n","tensor(113.0297, grad_fn=<AddBackward0>)\n","tensor(300.1332, grad_fn=<AddBackward0>)\n","tensor(104.9851, grad_fn=<AddBackward0>)\n","tensor(297.8704, grad_fn=<AddBackward0>)\n","tensor(103.3846, grad_fn=<AddBackward0>)\n","tensor(291.5779, grad_fn=<AddBackward0>)\n","tensor(103.2273, grad_fn=<AddBackward0>)\n","tensor(295.8019, grad_fn=<AddBackward0>)\n","tensor(110.3497, grad_fn=<AddBackward0>)\n","tensor(302.7369, grad_fn=<AddBackward0>)\n","tensor(119.0587, grad_fn=<AddBackward0>)\n","tensor(311.3109, grad_fn=<AddBackward0>)\n","tensor(110.4552, grad_fn=<AddBackward0>)\n","tensor(302.5617, grad_fn=<AddBackward0>)\n","tensor(96.4120, grad_fn=<AddBackward0>)\n","tensor(288.4242, grad_fn=<AddBackward0>)\n","tensor(106.3561, grad_fn=<AddBackward0>)\n","tensor(326.8235, grad_fn=<AddBackward0>)\n","tensor(106.7346, grad_fn=<AddBackward0>)\n","tensor(310.3525, grad_fn=<AddBackward0>)\n","tensor(92.9543, grad_fn=<AddBackward0>)\n","tensor(284.0383, grad_fn=<AddBackward0>)\n","tensor(97.9145, grad_fn=<AddBackward0>)\n","tensor(288.3267, grad_fn=<AddBackward0>)\n","tensor(108.8325, grad_fn=<AddBackward0>)\n","tensor(300.0882, grad_fn=<AddBackward0>)\n","tensor(111.6715, grad_fn=<AddBackward0>)\n","tensor(297.4485, grad_fn=<AddBackward0>)\n","tensor(118.4509, grad_fn=<AddBackward0>)\n","tensor(308.0004, grad_fn=<AddBackward0>)\n","tensor(104.9389, grad_fn=<AddBackward0>)\n","tensor(296.5460, grad_fn=<AddBackward0>)\n","Epoch 4, Batch 41 trainloss: 304.184570 loss: 296.545990\n","tensor(94.6262, grad_fn=<AddBackward0>)\n","tensor(284.0419, grad_fn=<AddBackward0>)\n","tensor(92.5092, grad_fn=<AddBackward0>)\n","tensor(283.0948, grad_fn=<AddBackward0>)\n","tensor(117.7334, grad_fn=<AddBackward0>)\n","tensor(309.1066, grad_fn=<AddBackward0>)\n","tensor(111.2600, grad_fn=<AddBackward0>)\n","tensor(300.2866, grad_fn=<AddBackward0>)\n","tensor(109.1413, grad_fn=<AddBackward0>)\n","tensor(299.2760, grad_fn=<AddBackward0>)\n","tensor(96.8976, grad_fn=<AddBackward0>)\n","tensor(283.2968, grad_fn=<AddBackward0>)\n","tensor(98.8979, grad_fn=<AddBackward0>)\n","tensor(289.5960, grad_fn=<AddBackward0>)\n","tensor(96.7679, grad_fn=<AddBackward0>)\n","tensor(286.5193, grad_fn=<AddBackward0>)\n","tensor(94.1612, grad_fn=<AddBackward0>)\n","tensor(281.8071, grad_fn=<AddBackward0>)\n","tensor(96.8494, grad_fn=<AddBackward0>)\n","tensor(304.4033, grad_fn=<AddBackward0>)\n","tensor(103.4392, grad_fn=<AddBackward0>)\n","tensor(294.2118, grad_fn=<AddBackward0>)\n","tensor(96.6843, grad_fn=<AddBackward0>)\n","tensor(282.0255, grad_fn=<AddBackward0>)\n","tensor(102.8208, grad_fn=<AddBackward0>)\n","tensor(291.9495, grad_fn=<AddBackward0>)\n","tensor(88.8250, grad_fn=<AddBackward0>)\n","tensor(275.1931, grad_fn=<AddBackward0>)\n","tensor(96.3941, grad_fn=<AddBackward0>)\n","tensor(279.2247, grad_fn=<AddBackward0>)\n","tensor(121.3934, grad_fn=<AddBackward0>)\n","tensor(307.7484, grad_fn=<AddBackward0>)\n","tensor(91.8126, grad_fn=<AddBackward0>)\n","tensor(279.8176, grad_fn=<AddBackward0>)\n","tensor(95.5556, grad_fn=<AddBackward0>)\n","tensor(284.5486, grad_fn=<AddBackward0>)\n","tensor(93.6638, grad_fn=<AddBackward0>)\n","tensor(279.6868, grad_fn=<AddBackward0>)\n","Epoch 5, Batch 1 trainloss: 279.686829 loss: 279.686829\n","tensor(100.7227, grad_fn=<AddBackward0>)\n","tensor(288.5739, grad_fn=<AddBackward0>)\n","tensor(86.7049, grad_fn=<AddBackward0>)\n","tensor(278.3442, grad_fn=<AddBackward0>)\n","tensor(109.6477, grad_fn=<AddBackward0>)\n","tensor(297.6094, grad_fn=<AddBackward0>)\n","tensor(99.5746, grad_fn=<AddBackward0>)\n","tensor(284.7830, grad_fn=<AddBackward0>)\n","tensor(100.9835, grad_fn=<AddBackward0>)\n","tensor(288.5793, grad_fn=<AddBackward0>)\n","tensor(108.5816, grad_fn=<AddBackward0>)\n","tensor(296.0156, grad_fn=<AddBackward0>)\n","tensor(101.1215, grad_fn=<AddBackward0>)\n","tensor(288.4290, grad_fn=<AddBackward0>)\n","tensor(102.0694, grad_fn=<AddBackward0>)\n","tensor(290.8479, grad_fn=<AddBackward0>)\n","tensor(115.8839, grad_fn=<AddBackward0>)\n","tensor(301.9138, grad_fn=<AddBackward0>)\n","tensor(116.3524, grad_fn=<AddBackward0>)\n","tensor(303.3653, grad_fn=<AddBackward0>)\n","tensor(104.6891, grad_fn=<AddBackward0>)\n","tensor(288.1106, grad_fn=<AddBackward0>)\n","tensor(99.3711, grad_fn=<AddBackward0>)\n","tensor(286.0203, grad_fn=<AddBackward0>)\n","tensor(117.4706, grad_fn=<AddBackward0>)\n","tensor(303.9425, grad_fn=<AddBackward0>)\n","tensor(113.6310, grad_fn=<AddBackward0>)\n","tensor(298.5576, grad_fn=<AddBackward0>)\n","tensor(94.4858, grad_fn=<AddBackward0>)\n","tensor(277.9764, grad_fn=<AddBackward0>)\n","tensor(101.2603, grad_fn=<AddBackward0>)\n","tensor(287.6407, grad_fn=<AddBackward0>)\n","tensor(110.5402, grad_fn=<AddBackward0>)\n","tensor(297.4913, grad_fn=<AddBackward0>)\n","tensor(109.5389, grad_fn=<AddBackward0>)\n","tensor(312.8140, grad_fn=<AddBackward0>)\n","tensor(112.3783, grad_fn=<AddBackward0>)\n","tensor(299.1598, grad_fn=<AddBackward0>)\n","tensor(113.3281, grad_fn=<AddBackward0>)\n","tensor(299.4486, grad_fn=<AddBackward0>)\n","Epoch 5, Batch 21 trainloss: 292.824280 loss: 299.448608\n","tensor(107.7682, grad_fn=<AddBackward0>)\n","tensor(294.6811, grad_fn=<AddBackward0>)\n","tensor(106.7562, grad_fn=<AddBackward0>)\n","tensor(290.5958, grad_fn=<AddBackward0>)\n","tensor(105.1291, grad_fn=<AddBackward0>)\n","tensor(290.5661, grad_fn=<AddBackward0>)\n","tensor(110.8524, grad_fn=<AddBackward0>)\n","tensor(321.6225, grad_fn=<AddBackward0>)\n","tensor(108.3217, grad_fn=<AddBackward0>)\n","tensor(287.5752, grad_fn=<AddBackward0>)\n","tensor(100.6201, grad_fn=<AddBackward0>)\n","tensor(285.4135, grad_fn=<AddBackward0>)\n","tensor(99.0893, grad_fn=<AddBackward0>)\n","tensor(279.3930, grad_fn=<AddBackward0>)\n","tensor(98.9406, grad_fn=<AddBackward0>)\n","tensor(283.4432, grad_fn=<AddBackward0>)\n","tensor(105.7633, grad_fn=<AddBackward0>)\n","tensor(290.0898, grad_fn=<AddBackward0>)\n","tensor(114.1059, grad_fn=<AddBackward0>)\n","tensor(298.3064, grad_fn=<AddBackward0>)\n","tensor(105.8683, grad_fn=<AddBackward0>)\n","tensor(289.9326, grad_fn=<AddBackward0>)\n","tensor(92.4207, grad_fn=<AddBackward0>)\n","tensor(276.3980, grad_fn=<AddBackward0>)\n","tensor(101.9464, grad_fn=<AddBackward0>)\n","tensor(313.1794, grad_fn=<AddBackward0>)\n","tensor(102.3107, grad_fn=<AddBackward0>)\n","tensor(297.4101, grad_fn=<AddBackward0>)\n","tensor(89.1142, grad_fn=<AddBackward0>)\n","tensor(272.2124, grad_fn=<AddBackward0>)\n","tensor(93.8666, grad_fn=<AddBackward0>)\n","tensor(276.3247, grad_fn=<AddBackward0>)\n","tensor(104.3258, grad_fn=<AddBackward0>)\n","tensor(287.5951, grad_fn=<AddBackward0>)\n","tensor(107.0470, grad_fn=<AddBackward0>)\n","tensor(285.0719, grad_fn=<AddBackward0>)\n","tensor(113.5425, grad_fn=<AddBackward0>)\n","tensor(295.1842, grad_fn=<AddBackward0>)\n","tensor(100.6019, grad_fn=<AddBackward0>)\n","tensor(284.2176, grad_fn=<AddBackward0>)\n","Epoch 5, Batch 41 trainloss: 291.427368 loss: 284.217621\n","tensor(90.7253, grad_fn=<AddBackward0>)\n","tensor(272.2452, grad_fn=<AddBackward0>)\n","tensor(88.6990, grad_fn=<AddBackward0>)\n","tensor(271.3428, grad_fn=<AddBackward0>)\n","tensor(112.8633, grad_fn=<AddBackward0>)\n","tensor(296.2648, grad_fn=<AddBackward0>)\n","tensor(106.6643, grad_fn=<AddBackward0>)\n","tensor(287.8211, grad_fn=<AddBackward0>)\n","tensor(104.6366, grad_fn=<AddBackward0>)\n","tensor(286.8581, grad_fn=<AddBackward0>)\n","tensor(92.9093, grad_fn=<AddBackward0>)\n","tensor(271.5556, grad_fn=<AddBackward0>)\n","tensor(94.8272, grad_fn=<AddBackward0>)\n","tensor(277.5949, grad_fn=<AddBackward0>)\n","tensor(92.7883, grad_fn=<AddBackward0>)\n","tensor(274.6523, grad_fn=<AddBackward0>)\n","tensor(90.2927, grad_fn=<AddBackward0>)\n","tensor(270.1427, grad_fn=<AddBackward0>)\n","tensor(92.8697, grad_fn=<AddBackward0>)\n","tensor(291.7950, grad_fn=<AddBackward0>)\n","tensor(99.1847, grad_fn=<AddBackward0>)\n","tensor(282.0363, grad_fn=<AddBackward0>)\n","tensor(92.7147, grad_fn=<AddBackward0>)\n","tensor(270.3659, grad_fn=<AddBackward0>)\n","tensor(98.5956, grad_fn=<AddBackward0>)\n","tensor(279.8787, grad_fn=<AddBackward0>)\n","tensor(85.1879, grad_fn=<AddBackward0>)\n","tensor(263.8291, grad_fn=<AddBackward0>)\n","tensor(92.4414, grad_fn=<AddBackward0>)\n","tensor(267.6965, grad_fn=<AddBackward0>)\n","tensor(116.3956, grad_fn=<AddBackward0>)\n","tensor(295.0306, grad_fn=<AddBackward0>)\n","tensor(88.0547, grad_fn=<AddBackward0>)\n","tensor(268.2737, grad_fn=<AddBackward0>)\n","tensor(91.6426, grad_fn=<AddBackward0>)\n","tensor(272.8113, grad_fn=<AddBackward0>)\n","tensor(89.8315, grad_fn=<AddBackward0>)\n","tensor(268.1575, grad_fn=<AddBackward0>)\n","Epoch 6, Batch 1 trainloss: 268.157501 loss: 268.157501\n","tensor(96.5968, grad_fn=<AddBackward0>)\n","tensor(276.6776, grad_fn=<AddBackward0>)\n","tensor(83.1663, grad_fn=<AddBackward0>)\n","tensor(266.8800, grad_fn=<AddBackward0>)\n","tensor(105.1522, grad_fn=<AddBackward0>)\n","tensor(285.3450, grad_fn=<AddBackward0>)\n","tensor(95.5014, grad_fn=<AddBackward0>)\n","tensor(273.0590, grad_fn=<AddBackward0>)\n","tensor(96.8531, grad_fn=<AddBackward0>)\n","tensor(276.7014, grad_fn=<AddBackward0>)\n","tensor(104.1358, grad_fn=<AddBackward0>)\n","tensor(283.8321, grad_fn=<AddBackward0>)\n","tensor(96.9886, grad_fn=<AddBackward0>)\n","tensor(276.5667, grad_fn=<AddBackward0>)\n","tensor(97.8986, grad_fn=<AddBackward0>)\n","tensor(278.8895, grad_fn=<AddBackward0>)\n","tensor(111.1390, grad_fn=<AddBackward0>)\n","tensor(289.4989, grad_fn=<AddBackward0>)\n","tensor(111.5899, grad_fn=<AddBackward0>)\n","tensor(290.8948, grad_fn=<AddBackward0>)\n","tensor(100.4142, grad_fn=<AddBackward0>)\n","tensor(276.2803, grad_fn=<AddBackward0>)\n","tensor(95.3193, grad_fn=<AddBackward0>)\n","tensor(274.2816, grad_fn=<AddBackward0>)\n","tensor(112.6673, grad_fn=<AddBackward0>)\n","tensor(291.4628, grad_fn=<AddBackward0>)\n","tensor(108.9894, grad_fn=<AddBackward0>)\n","tensor(286.3068, grad_fn=<AddBackward0>)\n","tensor(90.6420, grad_fn=<AddBackward0>)\n","tensor(266.5861, grad_fn=<AddBackward0>)\n","tensor(97.1365, grad_fn=<AddBackward0>)\n","tensor(275.8532, grad_fn=<AddBackward0>)\n","tensor(106.0325, grad_fn=<AddBackward0>)\n","tensor(285.2993, grad_fn=<AddBackward0>)\n","tensor(105.0747, grad_fn=<AddBackward0>)\n","tensor(299.9903, grad_fn=<AddBackward0>)\n","tensor(107.7980, grad_fn=<AddBackward0>)\n","tensor(286.9081, grad_fn=<AddBackward0>)\n","tensor(108.7102, grad_fn=<AddBackward0>)\n","tensor(287.1898, grad_fn=<AddBackward0>)\n","Epoch 6, Batch 21 trainloss: 280.793427 loss: 287.189819\n","tensor(103.3829, grad_fn=<AddBackward0>)\n","tensor(282.6249, grad_fn=<AddBackward0>)\n","tensor(102.4146, grad_fn=<AddBackward0>)\n","tensor(278.7138, grad_fn=<AddBackward0>)\n","tensor(100.8567, grad_fn=<AddBackward0>)\n","tensor(278.6900, grad_fn=<AddBackward0>)\n","tensor(106.3447, grad_fn=<AddBackward0>)\n","tensor(308.4643, grad_fn=<AddBackward0>)\n","tensor(103.9206, grad_fn=<AddBackward0>)\n","tensor(275.8325, grad_fn=<AddBackward0>)\n","tensor(96.5396, grad_fn=<AddBackward0>)\n","tensor(273.7649, grad_fn=<AddBackward0>)\n","tensor(95.0738, grad_fn=<AddBackward0>)\n","tensor(267.9982, grad_fn=<AddBackward0>)\n","tensor(94.9329, grad_fn=<AddBackward0>)\n","tensor(271.8853, grad_fn=<AddBackward0>)\n","tensor(101.4752, grad_fn=<AddBackward0>)\n","tensor(278.2617, grad_fn=<AddBackward0>)\n","tensor(109.4747, grad_fn=<AddBackward0>)\n","tensor(286.1433, grad_fn=<AddBackward0>)\n","tensor(101.5793, grad_fn=<AddBackward0>)\n","tensor(278.1202, grad_fn=<AddBackward0>)\n","tensor(88.6887, grad_fn=<AddBackward0>)\n","tensor(265.1491, grad_fn=<AddBackward0>)\n","tensor(97.8227, grad_fn=<AddBackward0>)\n","tensor(300.4162, grad_fn=<AddBackward0>)\n","tensor(98.1737, grad_fn=<AddBackward0>)\n","tensor(285.3029, grad_fn=<AddBackward0>)\n","tensor(85.5230, grad_fn=<AddBackward0>)\n","tensor(261.1494, grad_fn=<AddBackward0>)\n","tensor(90.0809, grad_fn=<AddBackward0>)\n","tensor(265.0965, grad_fn=<AddBackward0>)\n","tensor(100.1106, grad_fn=<AddBackward0>)\n","tensor(275.9068, grad_fn=<AddBackward0>)\n","tensor(102.7213, grad_fn=<AddBackward0>)\n","tensor(273.4922, grad_fn=<AddBackward0>)\n","tensor(108.9510, grad_fn=<AddBackward0>)\n","tensor(283.1924, grad_fn=<AddBackward0>)\n","tensor(96.5449, grad_fn=<AddBackward0>)\n","tensor(272.6819, grad_fn=<AddBackward0>)\n","Epoch 6, Batch 41 trainloss: 279.501190 loss: 272.681854\n","tensor(87.0763, grad_fn=<AddBackward0>)\n","tensor(261.2067, grad_fn=<AddBackward0>)\n","tensor(85.1348, grad_fn=<AddBackward0>)\n","tensor(260.3456, grad_fn=<AddBackward0>)\n","tensor(108.3069, grad_fn=<AddBackward0>)\n","tensor(284.2470, grad_fn=<AddBackward0>)\n","tensor(102.3644, grad_fn=<AddBackward0>)\n","tensor(276.1550, grad_fn=<AddBackward0>)\n","tensor(100.4217, grad_fn=<AddBackward0>)\n","tensor(275.2360, grad_fn=<AddBackward0>)\n","tensor(89.1777, grad_fn=<AddBackward0>)\n","tensor(260.5665, grad_fn=<AddBackward0>)\n","tensor(91.0183, grad_fn=<AddBackward0>)\n","tensor(266.3620, grad_fn=<AddBackward0>)\n","tensor(89.0646, grad_fn=<AddBackward0>)\n","tensor(263.5444, grad_fn=<AddBackward0>)\n","tensor(86.6728, grad_fn=<AddBackward0>)\n","tensor(259.2240, grad_fn=<AddBackward0>)\n","tensor(89.1454, grad_fn=<AddBackward0>)\n","tensor(279.9921, grad_fn=<AddBackward0>)\n","tensor(95.2029, grad_fn=<AddBackward0>)\n","tensor(270.6384, grad_fn=<AddBackward0>)\n","tensor(88.9996, grad_fn=<AddBackward0>)\n","tensor(259.4506, grad_fn=<AddBackward0>)\n","tensor(94.6410, grad_fn=<AddBackward0>)\n","tensor(268.5777, grad_fn=<AddBackward0>)\n","tensor(81.7837, grad_fn=<AddBackward0>)\n","tensor(253.1897, grad_fn=<AddBackward0>)\n","tensor(88.7416, grad_fn=<AddBackward0>)\n","tensor(256.9027, grad_fn=<AddBackward0>)\n","tensor(111.7171, grad_fn=<AddBackward0>)\n","tensor(283.1223, grad_fn=<AddBackward0>)\n","tensor(84.5371, grad_fn=<AddBackward0>)\n","tensor(257.4643, grad_fn=<AddBackward0>)\n","tensor(87.9796, grad_fn=<AddBackward0>)\n","tensor(261.8204, grad_fn=<AddBackward0>)\n","tensor(86.2439, grad_fn=<AddBackward0>)\n","tensor(257.3609, grad_fn=<AddBackward0>)\n","Epoch 7, Batch 1 trainloss: 257.360901 loss: 257.360901\n","tensor(92.7341, grad_fn=<AddBackward0>)\n","tensor(265.5368, grad_fn=<AddBackward0>)\n","tensor(79.8536, grad_fn=<AddBackward0>)\n","tensor(256.1434, grad_fn=<AddBackward0>)\n","tensor(100.9430, grad_fn=<AddBackward0>)\n","tensor(273.8586, grad_fn=<AddBackward0>)\n","tensor(91.6876, grad_fn=<AddBackward0>)\n","tensor(262.0782, grad_fn=<AddBackward0>)\n","tensor(92.9855, grad_fn=<AddBackward0>)\n","tensor(265.5760, grad_fn=<AddBackward0>)\n","tensor(99.9726, grad_fn=<AddBackward0>)\n","tensor(272.4200, grad_fn=<AddBackward0>)\n","tensor(93.1183, grad_fn=<AddBackward0>)\n","tensor(265.4550, grad_fn=<AddBackward0>)\n","tensor(93.9927, grad_fn=<AddBackward0>)\n","tensor(267.6872, grad_fn=<AddBackward0>)\n","tensor(106.6951, grad_fn=<AddBackward0>)\n","tensor(277.8685, grad_fn=<AddBackward0>)\n","tensor(107.1292, grad_fn=<AddBackward0>)\n","tensor(279.2119, grad_fn=<AddBackward0>)\n","tensor(96.4102, grad_fn=<AddBackward0>)\n","tensor(265.1967, grad_fn=<AddBackward0>)\n","tensor(91.5241, grad_fn=<AddBackward0>)\n","tensor(263.2834, grad_fn=<AddBackward0>)\n","tensor(108.1679, grad_fn=<AddBackward0>)\n","tensor(279.7697, grad_fn=<AddBackward0>)\n","tensor(104.6412, grad_fn=<AddBackward0>)\n","tensor(274.8278, grad_fn=<AddBackward0>)\n","tensor(87.0413, grad_fn=<AddBackward0>)\n","tensor(255.9131, grad_fn=<AddBackward0>)\n","tensor(93.2733, grad_fn=<AddBackward0>)\n","tensor(264.8075, grad_fn=<AddBackward0>)\n","tensor(101.8094, grad_fn=<AddBackward0>)\n","tensor(273.8739, grad_fn=<AddBackward0>)\n","tensor(100.8921, grad_fn=<AddBackward0>)\n","tensor(287.9722, grad_fn=<AddBackward0>)\n","tensor(103.5065, grad_fn=<AddBackward0>)\n","tensor(275.4259, grad_fn=<AddBackward0>)\n","tensor(104.3833, grad_fn=<AddBackward0>)\n","tensor(275.7005, grad_fn=<AddBackward0>)\n","Epoch 7, Batch 21 trainloss: 269.522247 loss: 275.700500\n","tensor(99.2737, grad_fn=<AddBackward0>)\n","tensor(271.3250, grad_fn=<AddBackward0>)\n","tensor(98.3463, grad_fn=<AddBackward0>)\n","tensor(267.5768, grad_fn=<AddBackward0>)\n","tensor(96.8531, grad_fn=<AddBackward0>)\n","tensor(267.5580, grad_fn=<AddBackward0>)\n","tensor(102.1203, grad_fn=<AddBackward0>)\n","tensor(296.1296, grad_fn=<AddBackward0>)\n","tensor(99.7960, grad_fn=<AddBackward0>)\n","tensor(264.8248, grad_fn=<AddBackward0>)\n","tensor(92.7154, grad_fn=<AddBackward0>)\n","tensor(262.8448, grad_fn=<AddBackward0>)\n","tensor(91.3104, grad_fn=<AddBackward0>)\n","tensor(257.3156, grad_fn=<AddBackward0>)\n","tensor(91.1766, grad_fn=<AddBackward0>)\n","tensor(261.0494, grad_fn=<AddBackward0>)\n","tensor(97.4558, grad_fn=<AddBackward0>)\n","tensor(267.1719, grad_fn=<AddBackward0>)\n","tensor(105.1335, grad_fn=<AddBackward0>)\n","tensor(274.7391, grad_fn=<AddBackward0>)\n","tensor(97.5587, grad_fn=<AddBackward0>)\n","tensor(267.0444, grad_fn=<AddBackward0>)\n","tensor(85.1904, grad_fn=<AddBackward0>)\n","tensor(254.6013, grad_fn=<AddBackward0>)\n","tensor(93.9569, grad_fn=<AddBackward0>)\n","tensor(288.4474, grad_fn=<AddBackward0>)\n","tensor(94.2951, grad_fn=<AddBackward0>)\n","tensor(273.9490, grad_fn=<AddBackward0>)\n","tensor(82.1564, grad_fn=<AddBackward0>)\n","tensor(250.7746, grad_fn=<AddBackward0>)\n","tensor(86.5317, grad_fn=<AddBackward0>)\n","tensor(254.5664, grad_fn=<AddBackward0>)\n","tensor(96.1582, grad_fn=<AddBackward0>)\n","tensor(264.9446, grad_fn=<AddBackward0>)\n","tensor(98.6652, grad_fn=<AddBackward0>)\n","tensor(262.6315, grad_fn=<AddBackward0>)\n","tensor(104.6454, grad_fn=<AddBackward0>)\n","tensor(271.9448, grad_fn=<AddBackward0>)\n","tensor(92.7406, grad_fn=<AddBackward0>)\n","tensor(261.8616, grad_fn=<AddBackward0>)\n","Epoch 7, Batch 41 trainloss: 268.323669 loss: 261.861572\n","tensor(83.6545, grad_fn=<AddBackward0>)\n","tensor(250.8523, grad_fn=<AddBackward0>)\n","tensor(81.7925, grad_fn=<AddBackward0>)\n","tensor(250.0296, grad_fn=<AddBackward0>)\n","tensor(104.0336, grad_fn=<AddBackward0>)\n","tensor(272.9732, grad_fn=<AddBackward0>)\n","tensor(98.3316, grad_fn=<AddBackward0>)\n","tensor(265.2108, grad_fn=<AddBackward0>)\n","tensor(96.4685, grad_fn=<AddBackward0>)\n","tensor(264.3326, grad_fn=<AddBackward0>)\n","tensor(85.6778, grad_fn=<AddBackward0>)\n","tensor(250.2567, grad_fn=<AddBackward0>)\n","tensor(87.4458, grad_fn=<AddBackward0>)\n","tensor(255.8229, grad_fn=<AddBackward0>)\n","tensor(85.5718, grad_fn=<AddBackward0>)\n","tensor(253.1223, grad_fn=<AddBackward0>)\n","tensor(83.2772, grad_fn=<AddBackward0>)\n","tensor(248.9792, grad_fn=<AddBackward0>)\n","tensor(85.6519, grad_fn=<AddBackward0>)\n","tensor(268.9168, grad_fn=<AddBackward0>)\n","tensor(91.4676, grad_fn=<AddBackward0>)\n","tensor(259.9427, grad_fn=<AddBackward0>)\n","tensor(85.5144, grad_fn=<AddBackward0>)\n","tensor(249.2076, grad_fn=<AddBackward0>)\n","tensor(90.9308, grad_fn=<AddBackward0>)\n","tensor(257.9723, grad_fn=<AddBackward0>)\n","tensor(78.5901, grad_fn=<AddBackward0>)\n","tensor(243.2047, grad_fn=<AddBackward0>)\n","tensor(85.2704, grad_fn=<AddBackward0>)\n","tensor(246.7727, grad_fn=<AddBackward0>)\n","tensor(107.3269, grad_fn=<AddBackward0>)\n","tensor(271.9457, grad_fn=<AddBackward0>)\n","tensor(81.2367, grad_fn=<AddBackward0>)\n","tensor(247.3187, grad_fn=<AddBackward0>)\n","tensor(84.5425, grad_fn=<AddBackward0>)\n","tensor(251.5040, grad_fn=<AddBackward0>)\n","tensor(82.8774, grad_fn=<AddBackward0>)\n","tensor(247.2266, grad_fn=<AddBackward0>)\n","Epoch 8, Batch 1 trainloss: 247.226578 loss: 247.226578\n","tensor(89.1092, grad_fn=<AddBackward0>)\n","tensor(255.0790, grad_fn=<AddBackward0>)\n","tensor(76.7449, grad_fn=<AddBackward0>)\n","tensor(246.0647, grad_fn=<AddBackward0>)\n","tensor(96.9925, grad_fn=<AddBackward0>)\n","tensor(263.0754, grad_fn=<AddBackward0>)\n","tensor(88.1083, grad_fn=<AddBackward0>)\n","tensor(251.7696, grad_fn=<AddBackward0>)\n","tensor(89.3556, grad_fn=<AddBackward0>)\n","tensor(255.1312, grad_fn=<AddBackward0>)\n","tensor(96.0649, grad_fn=<AddBackward0>)\n","tensor(261.7055, grad_fn=<AddBackward0>)\n","tensor(89.4856, grad_fn=<AddBackward0>)\n","tensor(255.0223, grad_fn=<AddBackward0>)\n","tensor(90.3263, grad_fn=<AddBackward0>)\n","tensor(257.1689, grad_fn=<AddBackward0>)\n","tensor(102.5233, grad_fn=<AddBackward0>)\n","tensor(266.9479, grad_fn=<AddBackward0>)\n","tensor(102.9416, grad_fn=<AddBackward0>)\n","tensor(268.2414, grad_fn=<AddBackward0>)\n","tensor(92.6513, grad_fn=<AddBackward0>)\n","tensor(254.7889, grad_fn=<AddBackward0>)\n","tensor(87.9613, grad_fn=<AddBackward0>)\n","tensor(252.9553, grad_fn=<AddBackward0>)\n","tensor(103.9434, grad_fn=<AddBackward0>)\n","tensor(268.7886, grad_fn=<AddBackward0>)\n","tensor(100.5586, grad_fn=<AddBackward0>)\n","tensor(264.0473, grad_fn=<AddBackward0>)\n","tensor(83.6607, grad_fn=<AddBackward0>)\n","tensor(245.8892, grad_fn=<AddBackward0>)\n","tensor(89.6459, grad_fn=<AddBackward0>)\n","tensor(254.4332, grad_fn=<AddBackward0>)\n","tensor(97.8438, grad_fn=<AddBackward0>)\n","tensor(263.1426, grad_fn=<AddBackward0>)\n","tensor(96.9644, grad_fn=<AddBackward0>)\n","tensor(276.6835, grad_fn=<AddBackward0>)\n","tensor(99.4763, grad_fn=<AddBackward0>)\n","tensor(264.6404, grad_fn=<AddBackward0>)\n","tensor(100.3197, grad_fn=<AddBackward0>)\n","tensor(264.9079, grad_fn=<AddBackward0>)\n","Epoch 8, Batch 21 trainloss: 258.938507 loss: 264.907867\n","tensor(95.4146, grad_fn=<AddBackward0>)\n","tensor(260.7099, grad_fn=<AddBackward0>)\n","tensor(94.5254, grad_fn=<AddBackward0>)\n","tensor(257.1143, grad_fn=<AddBackward0>)\n","tensor(93.0928, grad_fn=<AddBackward0>)\n","tensor(257.1000, grad_fn=<AddBackward0>)\n","tensor(98.1524, grad_fn=<AddBackward0>)\n","tensor(284.5407, grad_fn=<AddBackward0>)\n","tensor(95.9217, grad_fn=<AddBackward0>)\n","tensor(254.4827, grad_fn=<AddBackward0>)\n","tensor(89.1233, grad_fn=<AddBackward0>)\n","tensor(252.5847, grad_fn=<AddBackward0>)\n","tensor(87.7753, grad_fn=<AddBackward0>)\n","tensor(247.2784, grad_fn=<AddBackward0>)\n","tensor(87.6481, grad_fn=<AddBackward0>)\n","tensor(250.8676, grad_fn=<AddBackward0>)\n","tensor(93.6798, grad_fn=<AddBackward0>)\n","tensor(256.7512, grad_fn=<AddBackward0>)\n","tensor(101.0549, grad_fn=<AddBackward0>)\n","tensor(264.0224, grad_fn=<AddBackward0>)\n","tensor(93.7814, grad_fn=<AddBackward0>)\n","tensor(256.6360, grad_fn=<AddBackward0>)\n","tensor(81.9038, grad_fn=<AddBackward0>)\n","tensor(244.6889, grad_fn=<AddBackward0>)\n","tensor(90.3247, grad_fn=<AddBackward0>)\n","tensor(277.1988, grad_fn=<AddBackward0>)\n","tensor(90.6509, grad_fn=<AddBackward0>)\n","tensor(263.2781, grad_fn=<AddBackward0>)\n","tensor(78.9932, grad_fn=<AddBackward0>)\n","tensor(241.0238, grad_fn=<AddBackward0>)\n","tensor(83.1967, grad_fn=<AddBackward0>)\n","tensor(244.6692, grad_fn=<AddBackward0>)\n","tensor(92.4442, grad_fn=<AddBackward0>)\n","tensor(254.6408, grad_fn=<AddBackward0>)\n","tensor(94.8535, grad_fn=<AddBackward0>)\n","tensor(252.4228, grad_fn=<AddBackward0>)\n","tensor(100.5990, grad_fn=<AddBackward0>)\n","tensor(261.3719, grad_fn=<AddBackward0>)\n","tensor(89.1653, grad_fn=<AddBackward0>)\n","tensor(251.6901, grad_fn=<AddBackward0>)\n","Epoch 8, Batch 41 trainloss: 257.823883 loss: 251.690079\n","tensor(80.4389, grad_fn=<AddBackward0>)\n","tensor(241.1186, grad_fn=<AddBackward0>)\n","tensor(78.6514, grad_fn=<AddBackward0>)\n","tensor(240.3316, grad_fn=<AddBackward0>)\n","tensor(100.0169, grad_fn=<AddBackward0>)\n","tensor(262.3741, grad_fn=<AddBackward0>)\n","tensor(94.5410, grad_fn=<AddBackward0>)\n","tensor(254.9213, grad_fn=<AddBackward0>)\n","tensor(92.7525, grad_fn=<AddBackward0>)\n","tensor(254.0811, grad_fn=<AddBackward0>)\n","tensor(82.3881, grad_fn=<AddBackward0>)\n","tensor(240.5631, grad_fn=<AddBackward0>)\n","tensor(84.0876, grad_fn=<AddBackward0>)\n","tensor(245.9134, grad_fn=<AddBackward0>)\n","tensor(82.2885, grad_fn=<AddBackward0>)\n","tensor(243.3224, grad_fn=<AddBackward0>)\n","tensor(80.0853, grad_fn=<AddBackward0>)\n","tensor(239.3456, grad_fn=<AddBackward0>)\n","tensor(82.3677, grad_fn=<AddBackward0>)\n","tensor(258.5017, grad_fn=<AddBackward0>)\n","tensor(87.9559, grad_fn=<AddBackward0>)\n","tensor(249.8845, grad_fn=<AddBackward0>)\n","tensor(82.2377, grad_fn=<AddBackward0>)\n","tensor(239.5748, grad_fn=<AddBackward0>)\n","tensor(87.4425, grad_fn=<AddBackward0>)\n","tensor(247.9983, grad_fn=<AddBackward0>)\n","tensor(75.5876, grad_fn=<AddBackward0>)\n","tensor(233.8141, grad_fn=<AddBackward0>)\n","tensor(82.0066, grad_fn=<AddBackward0>)\n","tensor(237.2452, grad_fn=<AddBackward0>)\n","tensor(103.1985, grad_fn=<AddBackward0>)\n","tensor(261.4333, grad_fn=<AddBackward0>)\n","tensor(78.1333, grad_fn=<AddBackward0>)\n","tensor(237.7759, grad_fn=<AddBackward0>)\n","tensor(81.3105, grad_fn=<AddBackward0>)\n","tensor(241.8002, grad_fn=<AddBackward0>)\n","tensor(79.7117, grad_fn=<AddBackward0>)\n","tensor(237.6938, grad_fn=<AddBackward0>)\n","Epoch 9, Batch 1 trainloss: 237.693802 loss: 237.693802\n","tensor(85.7003, grad_fn=<AddBackward0>)\n","tensor(245.2415, grad_fn=<AddBackward0>)\n","tensor(73.8216, grad_fn=<AddBackward0>)\n","tensor(236.5835, grad_fn=<AddBackward0>)\n","tensor(93.2770, grad_fn=<AddBackward0>)\n","tensor(252.9311, grad_fn=<AddBackward0>)\n","tensor(84.7418, grad_fn=<AddBackward0>)\n","tensor(242.0714, grad_fn=<AddBackward0>)\n","tensor(85.9414, grad_fn=<AddBackward0>)\n","tensor(245.3045, grad_fn=<AddBackward0>)\n","tensor(92.3893, grad_fn=<AddBackward0>)\n","tensor(251.6248, grad_fn=<AddBackward0>)\n","tensor(86.0686, grad_fn=<AddBackward0>)\n","tensor(245.2063, grad_fn=<AddBackward0>)\n","tensor(86.8775, grad_fn=<AddBackward0>)\n","tensor(247.2721, grad_fn=<AddBackward0>)\n","tensor(98.5988, grad_fn=<AddBackward0>)\n","tensor(256.6722, grad_fn=<AddBackward0>)\n","tensor(99.0021, grad_fn=<AddBackward0>)\n","tensor(257.9185, grad_fn=<AddBackward0>)\n","tensor(89.1151, grad_fn=<AddBackward0>)\n","tensor(244.9951, grad_fn=<AddBackward0>)\n","tensor(84.6095, grad_fn=<AddBackward0>)\n","tensor(243.2362, grad_fn=<AddBackward0>)\n","tensor(99.9686, grad_fn=<AddBackward0>)\n","tensor(258.4545, grad_fn=<AddBackward0>)\n","tensor(96.7174, grad_fn=<AddBackward0>)\n","tensor(253.9017, grad_fn=<AddBackward0>)\n","tensor(80.4801, grad_fn=<AddBackward0>)\n","tensor(236.4556, grad_fn=<AddBackward0>)\n","tensor(86.2330, grad_fn=<AddBackward0>)\n","tensor(244.6693, grad_fn=<AddBackward0>)\n","tensor(94.1124, grad_fn=<AddBackward0>)\n","tensor(253.0423, grad_fn=<AddBackward0>)\n","tensor(93.2684, grad_fn=<AddBackward0>)\n","tensor(266.0582, grad_fn=<AddBackward0>)\n","tensor(95.6837, grad_fn=<AddBackward0>)\n","tensor(254.4884, grad_fn=<AddBackward0>)\n","tensor(96.4956, grad_fn=<AddBackward0>)\n","tensor(254.7489, grad_fn=<AddBackward0>)\n","Epoch 9, Batch 21 trainloss: 248.979538 loss: 254.748901\n","tensor(91.7827, grad_fn=<AddBackward0>)\n","tensor(250.7178, grad_fn=<AddBackward0>)\n","tensor(90.9294, grad_fn=<AddBackward0>)\n","tensor(247.2656, grad_fn=<AddBackward0>)\n","tensor(89.5539, grad_fn=<AddBackward0>)\n","tensor(247.2551, grad_fn=<AddBackward0>)\n","tensor(94.4179, grad_fn=<AddBackward0>)\n","tensor(273.6306, grad_fn=<AddBackward0>)\n","tensor(92.2752, grad_fn=<AddBackward0>)\n","tensor(244.7463, grad_fn=<AddBackward0>)\n","tensor(85.7424, grad_fn=<AddBackward0>)\n","tensor(242.9252, grad_fn=<AddBackward0>)\n","tensor(84.4479, grad_fn=<AddBackward0>)\n","tensor(237.8285, grad_fn=<AddBackward0>)\n","tensor(84.3267, grad_fn=<AddBackward0>)\n","tensor(241.2812, grad_fn=<AddBackward0>)\n","tensor(90.1253, grad_fn=<AddBackward0>)\n","tensor(246.9394, grad_fn=<AddBackward0>)\n","tensor(97.2153, grad_fn=<AddBackward0>)\n","tensor(253.9315, grad_fn=<AddBackward0>)\n","tensor(90.2254, grad_fn=<AddBackward0>)\n","tensor(246.8352, grad_fn=<AddBackward0>)\n","tensor(78.8098, grad_fn=<AddBackward0>)\n","tensor(235.3548, grad_fn=<AddBackward0>)\n","tensor(86.9052, grad_fn=<AddBackward0>)\n","tensor(266.6057, grad_fn=<AddBackward0>)\n","tensor(87.2198, grad_fn=<AddBackward0>)\n","tensor(253.2289, grad_fn=<AddBackward0>)\n","tensor(76.0152, grad_fn=<AddBackward0>)\n","tensor(231.8410, grad_fn=<AddBackward0>)\n","tensor(80.0568, grad_fn=<AddBackward0>)\n","tensor(235.3482, grad_fn=<AddBackward0>)\n","tensor(88.9471, grad_fn=<AddBackward0>)\n","tensor(244.9366, grad_fn=<AddBackward0>)\n","tensor(91.2642, grad_fn=<AddBackward0>)\n","tensor(242.8079, grad_fn=<AddBackward0>)\n","tensor(96.7885, grad_fn=<AddBackward0>)\n","tensor(251.4134, grad_fn=<AddBackward0>)\n","tensor(85.7986, grad_fn=<AddBackward0>)\n","tensor(242.1095, grad_fn=<AddBackward0>)\n","Epoch 9, Batch 41 trainloss: 247.940781 loss: 242.109467\n","tensor(77.4109, grad_fn=<AddBackward0>)\n","tensor(231.9502, grad_fn=<AddBackward0>)\n","tensor(75.6934, grad_fn=<AddBackward0>)\n","tensor(231.1966, grad_fn=<AddBackward0>)\n","tensor(96.2339, grad_fn=<AddBackward0>)\n","tensor(252.3898, grad_fn=<AddBackward0>)\n","tensor(90.9709, grad_fn=<AddBackward0>)\n","tensor(245.2283, grad_fn=<AddBackward0>)\n","tensor(89.2527, grad_fn=<AddBackward0>)\n","tensor(244.4237, grad_fn=<AddBackward0>)\n","tensor(79.2897, grad_fn=<AddBackward0>)\n","tensor(231.4311, grad_fn=<AddBackward0>)\n","tensor(80.9246, grad_fn=<AddBackward0>)\n","tensor(236.5775, grad_fn=<AddBackward0>)\n","tensor(79.1960, grad_fn=<AddBackward0>)\n","tensor(234.0895, grad_fn=<AddBackward0>)\n","tensor(77.0788, grad_fn=<AddBackward0>)\n","tensor(230.2692, grad_fn=<AddBackward0>)\n","tensor(79.2741, grad_fn=<AddBackward0>)\n","tensor(248.6884, grad_fn=<AddBackward0>)\n","tensor(84.6478, grad_fn=<AddBackward0>)\n","tensor(240.4074, grad_fn=<AddBackward0>)\n","tensor(79.1511, grad_fn=<AddBackward0>)\n","tensor(230.4984, grad_fn=<AddBackward0>)\n","tensor(84.1563, grad_fn=<AddBackward0>)\n","tensor(238.5999, grad_fn=<AddBackward0>)\n","tensor(72.7592, grad_fn=<AddBackward0>)\n","tensor(224.9653, grad_fn=<AddBackward0>)\n","tensor(78.9318, grad_fn=<AddBackward0>)\n","tensor(228.2672, grad_fn=<AddBackward0>)\n","tensor(99.3087, grad_fn=<AddBackward0>)\n","tensor(251.5266, grad_fn=<AddBackward0>)\n","tensor(75.2095, grad_fn=<AddBackward0>)\n","tensor(228.7829, grad_fn=<AddBackward0>)\n","tensor(78.2654, grad_fn=<AddBackward0>)\n","tensor(232.6551, grad_fn=<AddBackward0>)\n"]}]},{"cell_type":"code","source":["%load_ext rpy2.ipython"],"metadata":{"id":"_vYiSj4fUC-W"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["%%R\n","x <- seq(0, 2*pi, length.out=50)\n","x"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"AZaZkytvUJUL","executionInfo":{"status":"ok","timestamp":1659590712983,"user_tz":-480,"elapsed":788,"user":{"displayName":"洪睿甫","userId":"05593760976211923971"}},"outputId":"e4a5872d-8c24-4e1a-da10-fe06ad701f98"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":[" [1] 0.0000000 0.1282283 0.2564565 0.3846848 0.5129131 0.6411414 0.7693696\n"," [8] 0.8975979 1.0258262 1.1540544 1.2822827 1.4105110 1.5387393 1.6669675\n","[15] 1.7951958 1.9234241 2.0516523 2.1798806 2.3081089 2.4363372 2.5645654\n","[22] 2.6927937 2.8210220 2.9492502 3.0774785 3.2057068 3.3339351 3.4621633\n","[29] 3.5903916 3.7186199 3.8468481 3.9750764 4.1033047 4.2315330 4.3597612\n","[36] 4.4879895 4.6162178 4.7444460 4.8726743 5.0009026 5.1291309 5.2573591\n","[43] 5.3855874 5.5138157 5.6420439 5.7702722 5.8985005 6.0267288 6.1549570\n","[50] 6.2831853\n"]}]},{"cell_type":"code","source":["%%R\n","CData <- readRDS(\"C.Diagnosis_AllClean.rds\") %>% ungroup\n","CData"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"pR9WQXNeUMKK","executionInfo":{"status":"ok","timestamp":1659591537255,"user_tz":-480,"elapsed":1982,"user":{"displayName":"洪睿甫","userId":"05593760976211923971"}},"outputId":"8ed5158b-a6fb-4669-db37-0c563e76b4bf"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["# A tibble: 159,511 × 16\n","   LOC   IDCODE   CNRSEQ Re.Ty FISTRECTYPE FISTRECDATE TMRPRMYST CANCERKD SYMOGN\n","   <chr> <chr>    <chr>  <chr> <chr>       <chr>       <chr>     <chr>    <chr> \n"," 1 2     002865E… 02     META  70          00000000    C185      大腸直…  0     \n"," 2 3     00C2255… 01     META  70          00000000    C739      甲狀癌   0     \n"," 3 3     011DB24… 01     META  70          00000000    C220      肝癌     0     \n"," 4 6     0181DE7… 01     NO    00          00000000    C209      大腸直…  0     \n"," 5 2     019E95D… 01     META  70          00000000    C220      肝癌     0     \n"," 6 2     01B3044… 01     NO    00          00000000    C508      乳癌     2     \n"," 7 2     01B3044… 02     NO    00          00000000    C504      乳癌     1     \n"," 8 3     01B3044… 02     NO    00          00000000    C508      乳癌     1     \n"," 9 6     01F26EB… 01     META  70          00000000    C343      肺癌     1     \n","10 3     0235BEE… 01     NO    00          00000000    C508      乳癌     2     \n","# … with 159,501 more rows, and 7 more variables: Lu_CLNCDXDAT <date>,\n","#   Lu_PTHLDXDAT <date>, CNRDXAGE <chr>, LIFSTS <chr>, FullDate <date>,\n","#   Gender <chr>, LastDate <date>\n"]}]}]}